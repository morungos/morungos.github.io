---
chapter: 4
title: "'Big Science'"
layout: chapter
sitemap: false
---

Nothing demonstrates the impacts of reflexive modernization as clearly as
science. Historically, science was the realm of independently wealthy gentlemen,
controlled through universities and other institutions, such as the Royal
Society. But as modernization started to get up a head of steam, 

That relationship was transformed during the middle of the 20th Century, 
particularly through war. War demanded that science provide new weapons and
new advantages. War threw the whole resource of the state and the government
into developing those weapons and advantages. Inevitably, that changed the
way that science worked. 

Building on that, and in the aftermath of the wars, a new, 'modern' approach to
science began to emerge, a truly modern 'Big Science'. In Table 1 below, I've
sketched out the typical characteristics that distinguish a Big Science approach
from a traditional ("Little Science") one. Of course, these differences are not
always clear: there is a big blurry area between them. Nevertheless, there's
evidence of a long-term drift towards a 'Big Science culture' in many STEM
fields, such that a large proportion of the work is pulled in that
direction[^Initiatives]. Funding calls today may be focused on grand challenges,
for example, or the creation of collaborative networks.

[^Initiatives]: I'm consciously excluding umbrella-style initiatives here. For
    example, the EU's Horizon 2020 and the NSF National AI Research Institutes
    are not typical Big Science, for several reasons: they're often distributed,
    and there isn't the unity of purpose. Research consortia formed through
    these initiatives may be partly shaped by these forces, but they aren't
    typical of them.

| "Big Science"  | "Little Science" |
|-----------|----------|
| Centralized | Distributed |
| Collaboration-centred | Individual-centred |
| Integrated into the economy | Independent from the economy |
| High brand value | Low brand value |
| > $1B funding | Modest funding |
| One grand challenge | Multiple, targeted questions |
| Planned | Responsive |
| Communication by press release | Communication by academic article |
| Applied research | Pure research |
| Attitude of confidence, belief | Attitude of skepticism, questioning |
| Consequentialist ethics | Virtue ethics |
| Guided by theory | Guided by experiment |
{:.w-100.table.table-sm.table-striped}

**Table 1. Typical characteristics of Big Science and Little Science**
{:.text-muted.}

At first, Big Science was simply a modernized science, applying industrial
efficiencies to science, and especially its methods. This was the era of growth
in doctoral training, of the development of an established scientific literature
and publishing systems. Research became a profession, increasingly decoupled 
from teaching.

Computers, obviously, were a fundamental part of that: computational methods
spread across all the sciences. 

But the point about Big Science is that it is more than an industrialized
science. The typical characteristics above show that: Big Science is integrated
into the economy, and applied. It centralizes science enough that the concepts
of an "academic community" start to dissolve -- communications are targeted at
other actors: governments, the media, industry. Big Science is different because
it changes things, and especially industry and governments, in ways that Little
Science only affected indirectly. Big Science can write policy, where Little
Science could only inform it.

Harry Collins (2003) uses a richer classification, distinguishing "centralized
big science" from "federal big science", with an overlapping mixed category
showing aspects of both. It's a fascinating and remarkable history of the
evolution of one particular example, the Laser Interferometer Gravitational-Wave
Observatory (LIGO). I'd very much recommend reading it, as a beautiful
illustration of how the culture and structure of the organization affects the
science that is done. Big Science -- however it is done -- transforms the way we
work together, and through that, what we do.

## The Big Science of artificial intelligence

Tradition says that artificial intelligence truly began in June 1956, at a
summer research workshop convened by John McCarthy and held at Dartmouth
College, New Hampshire. The attendees of the workshop — all men — included
physicists, engineers, scientists, mathematicians, and neuroscientists. At this
time, general purpose computers as we know them today had only existed for about
five years. Programming languages were only a couple of years old. 

At the time, what was to become artificial intelligence didn’t even have a name.
Contemporary notes referred to “synthetic intelligence”, the preferred term in
the UK (where Alan Turing was based, and which was exploring parallel ideas) was
“machine intelligence”. Before the Dartmouth workshop, AI was very much rooted
in cybernetics. In fact, convergence on term “artificial intelligence” as at
least as much a way to distance the field from the giants of cybernetics, such
as Norbert Wiener. 

But that’s just one story. 

Six years earlier, in 1950, Alan Turing had published Computing Machinery and
Intelligence, a landmark paper that asked the question: “can machines think?”
And two years before that in 1948, his Intelligent Machinery laid out a vision
for universal, digital computers implementing a learning system with explicit
analogies to the human cortex and to neural network models. Turing’s work is
less ‘symbolic’ than McCarthy’s, but perhaps — through his concept of what
became the Turing test — every bit as influential on the future unfolding of the
field. In retrospect, Turing’s vision of machine learning as essential to
machine intelligence might have been yet more prescient.

In fact, this UK incarnation of machine intelligence largely grew out of the
“Ratio Club” — which was (unlike in the US) firmly rooted in cybernetics. The
Ratio Club[^RatioClub] — a gentlemen's dining club which formally excluded both
full professors and women — included many innovative thinkers with an interest
in cybernetic approaches, such as Grey Walter and Ross Ashby.

[^RatioClub]: A parallel thread in the US was the Macy-funded cybernetics
    conferences — and there was a significant overlap and cross-communication.
    For example, Warren McCulloch presented to the Ratio Club, and Donald
    Mackay, Ross Ashby, and Grey Walter to the Macy conferences.

In fact, there’s a case for saying that US computing grew out of simulation,
where UK computing grew out of cryptography, substantially as a consequence of
the different stresses from their respective war efforts.

And yet, that’s not the only story either. 

## AI winters

All these stories are modernist. They are accounts of a field that is always
moving *forwards*, maybe in dramatic surges. And that does not match the later
histories of artificial intelligence. Across the world, several times, there
have been several “AI Winters” where research work essentially stops, at least
for a time. In these phases, there is no progress, because artificial
intelligence has failed to deliver.

There are several different stories about how these AI winters happen.

### AI winters as hype cycles

The first explanation is the most obvious — that artificial intelligence was
largely hype, and the winters were when people started to see through through
the bullshit. There could then be a delay, while people forgot about the hype,
and developed new narratives to persuade investors, both from funding agencies
and corporations. There is likely some truth to this — researchers were
competing to justify their work to research funding agencies, and it would be 
surprising if they didn't present their work in the best possible light.

### AI winters as Kuhnian crises

Perhaps the most common explanation is based on Thomas Kuhn’s: that these were
periods of crisis within the field, and it required new theories to come to hand
to cause a revolution, a paradigm shift, before further progress is possible.
There is some sense in this, but the timing feels off. For example, the current
activity in AI is strongly tied to more complex neural network topologies, such
as convolutional neural networks. Not only did these very much exist before the
AI winter of 1990 (the first paper on them was published in 1987), but the
actual re-invigoration happened very much later, around 2010, and was tied more
to innovative use of GPUs and custom hardware to make those early convolutional
neural networks viable on affordable hardware. But this was a methodological
innovation, not a theoretical one, which doesn’t really fit Kuhn’s thesis. In
fact, look as you might, it is very hard to see any innovations across any AI
winter which were not firmly rooted in ideas that were well-established before
that downturn. 

### AI winters as business cycles

Unlike the Kuhnian explanation, which argues that AI winters are purely internal
to the field, a second explanation is that AI winters are driven by external
factors. For example, the 1973 AI winter coincided with the 1973 oil crisis and
stagflation. The second coincided with Black Monday, the 1987 stock market
crash. 

### AI winters as hardware value cycles

Finally, AI winters also coincided with major hardware innovations. The first
microprocessors were launched in the early 1970s, starting to eat into the
mainframe and minicomputer markets, and the late 1980s saw the transition from
custom AI hardware (notably Lisp machines) to much more affordable workstations. 

## The alternative: bricolage in science

Not all science is Big Science. Even Alvin Weinberg, arguably its biggest
advocate, was explicit that "We must make Big Science flourish without, at the
same time, allowing it to trample Little Science" (Weinberg, 1961).

However, we may want to add one more item to Table 1's categorization of the
differences between Big Science and Little Science: where Big Science uses
engineering, Little Science uses *bricolage*.

Bricolage (a loan-word from French, roughly equivalent to the English "DIY" or
"tinkering"[^Hacking]) was introduced as a concept by Claude Lévi-Strauss in *The Savage
Mind*. He uses it to describe how conceptual structures are put together, piece
by piece.

[^Hacking]: The original positive meaning of the term "hacking" in computing was
    an almost perfect version of bricolage: discovery through tinkering to
    understand and solve challenging technical problems. An example: using an 
    old mainframe line printer to generate music. 

Bricolage, put simply, means playing around, trying ideas, and testing them.
It's an effective problem-solving strategy, although more so in the absence of
guiding knowledge. Engineering, by contrast, is thinks about goals, and means to
an end, i.e., using knowledge first.

Unhelpfully for us, Lévi-Strauss contrasts bricolage with "science" more than
"engineering". Essentially, his distinction is that the 'bricoleur' pieces
together conceptual structures from (often second-hand) observations, bottom-up,
where the scientist interprets observations from conceptual structures,
top-down. Although, as Lévi-Strauss himself put it, "both approaches are equally
valid" (Lévi-Strauss, 1962, p22). What is confusing for us is that
Lévi-Strauss's use of "science" is more of a myth than a reality. If we look at
[Goldstein's distinction between "science" and "principled machine
learning"](https://twitter.com/tomgoldsteincs/status/1484609273162309634), it
exactly matches Lévi-Strauss's, except that science has 'reversed its polarity'
-- Goldstein's science, and ours, especially Little Science, happen
substantially through bricolage -- and therefore *align with* science, rather
than opposing it in Lévi-Strauss's original usage. For these reasons, we can
contrast bricolage more with engineering, where there is a more 'principled'
construction -- and after all, contrasting 'do-it-yourself' with engineering does make
more sense.

In many ways, it is better to use 
[Seymour Papert's 1993 adaptation of bricolage](https://lcl.media.mit.edu/resources/readings/childrens-machine.pdf)
as a method for building mental constructs, i.e., *learning*. Learning
happens through bricolage, and whether it is creating new concepts and 
methods in science, or new concepts and methods in a child's mind, the
process is virtually the same. For example, if we revisit the later parts of [Story 3](#story3) through
the lens of bricolage, the "do it yourself" assembly and re-purposing of 
tools and ideas is extremely clear.

The problem is: bricolage doesn't work well in Big Science. It can't. When you are building 
something the scale of GPT-3, the Human Genome Project, the LHC, or the Manhattan Project, 
experiments have to be few and far between. The scale, and the cost, simply make 
tinkering unacceptable or uneconomic, and usually both. 
