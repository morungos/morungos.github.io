---
chapter: 3
title: "'Big Science'"
layout: chapter
sitemap: false
---

Tradition says that artificial intelligence truly began in June 1956, at a
summer research workshop convened by John McCarthy and held at Dartmouth
College, New Hampshire. The attendees of the workshop — all men — included
physicists, engineers, scientists, mathematicians, and neuroscientists. At this
time, general purpose computers as we know them today had only existed for about
five years. Programming languages were only a couple of years old. 

At the time, what was to become artificial intelligence didn’t even have a name.
Contemporary notes referred to “synthetic intelligence”, the preferred term in
the UK (where Alan Turing was based, and which was exploring parallel ideas) was
“machine intelligence”. Before the Dartmouth workshop, AI was very much rooted
in cybernetics. In fact, convergence on term “artificial intelligence” as at
least as much a way to distance the field from the giants of cybernetics, such
as Norbert Wiener. 

But that’s just one story. 

Six years earlier, in 1950, Alan Turing had published Computing Machinery and
Intelligence, a landmark paper that asked the question: “can machines think?”
And two years before that in 1948, his Intelligent Machinery laid out a vision
for universal, digital computers implementing a learning system with explicit
analogies to the human cortex and to neural network models. Turing’s work is
less ‘symbolic’ than McCarthy’s, but perhaps — through his concept of what
became the Turing test — every bit as influential on the future unfolding of the
field. In retrospect, Turing’s vision of machine learning as essential to
machine intelligence might have been yet more prescient.

In fact, this UK incarnation of machine intelligence largely grew out of the
“Ratio Club” — which was (unlike in the US) firmly rooted in cybernetics. The
Ratio Club[^RatioClub] — a gentlemen's dining club which formally excluded both
full professors and women — included many innovative thinkers with an interest
in cybernetic approaches, such as Grey Walter and Ross Ashby.

[^RatioClub]: A parallel thread in the US was the Macy-funded cybernetics
    conferences — and there was a significant overlap and cross-communication.
    For example, Warren McCulloch presented to the Ratio Club, and Donald
    Mackay, Ross Ashby, and Grey Walter to the Macy conferences.

In fact, there’s a case for saying that US computing grew out of simulation,
where UK computing grew out of cryptography, substantially as a consequence of
the different stresses from their respective war efforts.

And yet, that’s not the only story either. 

## AI winters

All these stories are modernist. They are accounts of a field that is always
moving *forwards*, maybe in dramatic surges. And that does not match the later
histories of artificial intelligence. Across the world, several times, there
have been several “AI Winters” where research work essentially stops, at least
for a time. In these phases, there is no progress, because artificial
intelligence has failed to deliver.

There are several different stories about how these AI winters happen.

### AI winters as hype cycles

The first explanation is the most obvious — that artificial intelligence was
largely hype, and the winters were when people started to see through through
the bullshit. There could then be a delay, while people forgot about the hype,
and developed new narratives to persuade investors, both from funding agencies
and corporations. There is likely some truth to this — researchers were
competing to justify their work to research funding agencies, and it would be 
surprising if they didn't present their work in the best possible light.

### AI winters as Kuhnian crises

Perhaps the most common explanation is based on Thomas Kuhn’s: that these were
periods of crisis within the field, and it required new theories to come to hand
to cause a revolution, a paradigm shift, before further progress is possible.
There is some sense in this, but the timing feels off. For example, the current
activity in AI is strongly tied to more complex neural network topologies, such
as convolutional neural networks. Not only did these very much exist before the
AI winter of 1990 (the first paper on them was published in 1987), but the
actual re-invigoration happened very much later, around 2010, and was tied more
to innovative use of GPUs and custom hardware to make those early convolutional
neural networks viable on affordable hardware. But this was a methodological
innovation, not a theoretical one, which doesn’t really fit Kuhn’s thesis. In
fact, look as you might, it is very hard to see any innovations across any AI
winter which were not firmly rooted in ideas that were well-established before
that downturn. 

### AI winters as business cycles

Unlike the Kuhnian explanation, which argues that AI winters are purely internal
to the field, a second explanation is that AI winters are driven by external
factors. For example, the 1973 AI winter coincided with the 1973 oil crisis and
stagflation. The second coincided with Black Monday, the 1987 stock market
crash. 

### AI winters as hardware value cycles

Finally, AI winters also coincided with major hardware innovations. The first
microprocessors were launched in the early 1970s, starting to eat into the
mainframe and minicomputer markets, and the late 1980s saw the transition from
custom AI hardware (notably Lisp machines) to much more affordable workstations. 

