---
chapter: 3
title: Reflexive modernity
layout: chapter
sitemap: false
---

For the past five centuries or so, particularly in the Western
world[^WesternWorld], there was a clear break from the medieval world, with its
strict hierarchies and agricultural economies. The modern world did not arrive
all at once, there were distinct stages —or surges — of development, that piece
by piece transformed the world into the modern, industrialized, interconnected
one that we see today. Generally speaking, so far there have been three phases
of modernization.

[^WesternWorld]: Yes, this is a more-than-a-little colonialist way of
    summarizing it — perhaps inevitably, given that colonialism was itself a
    product of this process of modernization. It is likely that the key phases
    were directly shaped by forces elsewhere — in particular, the Islamic Golden
    Age presaged early modernization by over a hundred years. Instead, the
    actual “Great Divergence” globally was primarily an effect of the classical
    modernization phase, not the early or late phases.

#### Early modernization (from around 1500 to the late 1700s)

Early modernization started with the Renaissance, the beginnings of global
connections across borders, and early experimental science. The power of
religious organizations was gradually eroded and replaced — for the most part —
with centralized governments. Technology was beginning to have an impact by the
end of the period, but the use of scientific knowledge to reorganize day to day
life was still in its infancy.

#### ‘Classical’ modernization (from the late 1700s to around 1900)

At this point, the Industrial Revolution itself took hold, and a recognizable world economy with global trading emerged. Governments began to shift towards democracies, and social movements criticizing them started to gain momentum. Technological innovations in transport and industry enabled urban centres to crystallize, and life within them to shift away from agricultural precursors into modern industrialized cities.

#### Late modernization (from around 1900 to 1989)

In late modernization, true globalization appeared, enabled by transformative communications technologies and transport systems. Manufacturing and services became global — as did wars and crises. Individual nation states started to decline in importance. Economic development took centre stage for policies, and global organizations like the WTO and the United Nations emerged and were major influences alongside other governing bodies.

---

A common theme throughout this whole modernization process is the theme of
*progress*. This was a marked break from the medieval world, where people were the
subjects of God, nature, and our lords and masters. In the medieval world, what
technological innovation there was (not much!, but one important example was the
mechanical clock) was firmly under the control of the Church and the nobles.
Modernization changed all this — progress itself became a Good Thing. We could
start to shape our environment, make a new and better world, improved over the
one that we inherited from our forbears. Along the way, we developed
technologies that both helped us shape the world — examples include the steam
engine, trains, industrial automation, electricity, the telegraph, radio — and
that, in turn, laid the foundations for the innovations yet to come.

Computers, when they finally came along, were the product as well as the tool of
modernization. Before the modern electronic computers we know today, the word
“computer” was a job title. Early computers, usually women, were people who
followed the steps of a mathematical process to implement computation at scale.
They were the ones who created the tables of logarithms and trigonometric
functions that enabled people to make calculations more efficient and more
accurate. The computers we know today are their replacements — they literally
simulate these human computers. As Bailey (1992) put it: ”Von Neumann and
Goldstine were not inventing the computer. They already knew what a computer
was, what a computer did, and how long it took a computer to do it.” So,
computers themselves are a perfect example of reflexive modernization — what was
created as a product of modern science, with very specific goals of improving
particular tasks, begins to transform everything else, including the society
that had created them in the first place. Although it might seem that no single
technology has been as pivotally influential as the computer, that’s not
strictly true, as these technologies come in layers, and computers themselves
build on the earlier innovations of electricity, switching, and communications.
And in the same way, artificial intelligence is another layer of transformation
on top of all of them. And it won’t be the last, I can promise you that. 

Although these three phases of modernization are recognizable and relatively
clear with the benefit of hindsight, there are different opinions on what
happened next. What is clear is that around the late 1970s, there was another
significant shift. Manufacturing industry started to decline in favour of
services, and there was a dramatic acceleration in the importance of knowledge
to the economy. It is very easy to personalize this as a policy decision by
those in power at the time — it was important to both Reagan’s and Thatcher’s
platforms. 

But another possibility is that the modern world literally came to an end, and
we entered a new, *post-modern* age. The other possibility is that, instead,
modernization accelerated but, if you like, turned inward rather than outward,
into a new phase of reflexive modernity. 

## The modernizing project breaks down: postmodernism

In the later stages of modernity, all was not well. Cracks were starting to show
in the progress — or, rather, unexpected side effects started to have surprising
effects. So, from the middle of the 20th century, in a wide range of fields,
there was an increasing question over whether modernism and its gradual
progressive improvement could be sustained. Especially after the 1950s, what had
then seemed to be an infinite source of optimism began to dry up. 

In language, for example, following Wittgenstein, words and categories no longer
seemed to have a solid basis in reality. Where, before, words seemed to be
precise tools to communicate from a writer to a reader, references to essential
characteristics of a real, objective world, that all started to fall apart. When
we talk about the colour, red, for example, there is no essence-of-redness;
instead, there is a ‘typical’ red, and there are crimson, ochre, fuchsia, brown,
purple, which may or may not be red, at least partly depending on context. A lot
more of the world was turning out to be socially constructed, defined by
agreement and negotiation between people, rather than based on an objective real
world. 

Similarly, in ethics, we moved from a world where good and bad were clearly
distinguishable into a world filled with nuance and context, where nothing is
objectively right and nothing objectively wrong. Privacy is a good example:
there are genuine instances where absolute privacy can be harmful, and where
invading privacy can bring justice to those who need it. And yet, that exact
same loss of privacy can be intensely harmful in other instances. 

The result was a growing coalition of work around the end of the modern world —
postmodernism. This has a number of characteristics which distinguish it from
its modern predecessor.

*   **From objective to relative truth.** In postmodernism, there are no universal
	objective truths at all. Instead concepts such as redness are socially
	constructed. That doesn’t necessarily mean a complete free-for-all, but it
	does mean that culture, consensus, and other factors go into the pot. What
	redness is, is not completely arbitrary, but it doesn’t have pure essence
	either. Instead, we agree what it is as a society.[^SocialConstruct]

*   **Knowledge depends on context,** especially from politics and culture.
	Because these social factors play into knowledge, the boundaries become
	blurry, contested, and might be different for each of us. What I think is
	red, you might think is not red — maybe if you’re a professional artist with
	more subtle colour training, for example.

*   **Reflexivity.** A common characteristic of postmodernism is that it is
	reflexive or self-referential — it is particularly playful with concepts of
	representation. Recursion is a common theme, so are maps and indexes. 

[^SocialConstruct]: The distinction between “objective” on the one hand, and
    “socially constructed” on the other, is not always useful. Take the meter
    length unit, for instance. The meter is a social construct in the sense that
    it was created by consensus, even though it has evolved through several very
    different, objective, specifications, created by bodies such as the French
    National Assembly and the International Committee for Weights and Measures.
    So, it is both objective and a social construct. In effect, social
    construction is how something was established, not what was established.

The challenge, of course, is when to stop. If everything is relative, if there
is no grounding in any ‘real’ world, then surely, anything does. I can call
green ‘red’ if I want — and who can stop me? In the limit, radical postmodernism
seems to descend into a kind of reflexive absence of meaning. After all — if
words and concepts don’t have a pure essence of meaning to them, if they aren’t
in anyone’s head but instead are part of our social world, how is it possible to
modernize things like expertise, that appear to be properties of individuals?
Even small steps towards postmodernism — if taken to its limit as a radical
position — can feel like a slippery slope into a chaos where there is no truth,
no meaning, no representation. 

That isn’t necessarily the case, though: the virtue of postmodernism is that it
exposes the cracks in the modernist facade. It doesn’t need to provide
explanations about how to overcome them to be useful. It opens the curtain
enough to reveal glimpses about what’s going on behind — and this is where its
reflexivity becomes truly insightful. 

It is no accident that postmodernism is particularly influential in the arts —
perhaps where commentary on society is so free. A lovely example is the movie,
*Paint Drying* — which is literally of paint drying on a wall for over ten
hours. Except it isn’t about that at all: it was created as a protest of film
rating systems, and the way small independent creators were billed for it. So
the movie is ironic, and more about censorship than about paint. 

# Modernization in practice: Guinness

One of the best examples of the process of modernization is the innovation in
the brewing company, Guinness, and especially the involvement of the man who
eventually became one of their Head Brewers, one William Sealy Gosset. 

Brewing was an industry that modernized early. In the Middle Ages most beer was
brewed by monasteries. Over time, this role was increasingly undertaken
initially by guilds with government monopolies, such as London’s Worshipful
Company of Brewers, and into the 19^th^ century, increasingly by corporations
like Guinness and Shepherd Neame in the UK, and Yuengling and Pabst in the US.
These new companies drew increasingly on science, using innovations like hopping
(which preserved the beer, keeping it safer longer) and hygrometers (to control
the amount of sugar, keeping the product reliable). For example, the main trade
association in the UK, the Institute of Brewing and Distilling, was originally
founded in 1886 as the Laboratory Club, because it integrated brewing and
science. 

Gosset was a remarkably strong (although self-trained) statistician -- a man who
corresponded in depth with the leading lights of the statistical
movement[^StatisticsEugenics]. Unlike them, Gosset remained solidly focused on
his day job: improving practice at Guinness. 

[^StatisticsEugenics]: Over the years, Gosset corresponded and worked with both
    Karl Pearson and Ronald Fisher, among the two most important figures in 20th
    Century statistics — and, perhaps, the clearest examples of its distasteful
    history (both also were leaders in the eugenics movement). Pearson and
    Fisher hated each other, but both respected Gosset. There is no evidence
    that Gosset endorsed eugenics.

When Gosset started out in the brewing industry, brewing was still rooted in
traditional expertise. Human judgement shaped every step of the process from
what varieties of grain to grow, when to harvest, through to fermentation
temperatures and conditions. This was all to change as the process of
modernization got to work. 

As part of the transformation that Gosset pioneered during his career at
Guinness, he developed a wide range of techniques and methods for using
mathematics and experimental methods to directly improve commercial success.
Many of those methods are still used today in modern statistics, where Gosset is
better known by his pseudonym, “Student”[^Student].

[^Student]: Gosset published several important papers under the name, “Student”. His
    employers at Guinness were happy for him to publish, under condition that it
    was not linked to Guinness.

Let’s pick an example from one part of the process. Brewing uses malted barley
as its main source of the sugars which drive fermentation. If there are a lot of
sugars in the malt, the resulting product has more alcohol in it. But Guinness
wanted to make their product consistently, no matter what. There were sound
commercial reasons to get this just right: if the alcohol content was high, the
beer would last longer, but it would attract a higher government tax; and if too
low, the beer would spoil more quickly, and have to be thrown out too soon. So,
where traditionally this relied on the expertise to estimate the sugars, Gosset
and his bosses wanted to measure the sugar content reliably, to hit the sweet
spot in the middle, and hit it reliably every single time, because that would
reduce their costs and therefore boost their profits. 

Unfortunately, a single measurement of the amount of sugar in a sample tends not
to be very reliable. Repeating the measurements provides much more data, and
improves the reliability. Gusset’s innovation was to mathematically calculate
exactly how to combine the results of experiments in way that optimizes
confidence in the result. In effect, Gosset was able to work out exactly how
many assessments were needed to be certain that the amount of sugar was perfect
for fermentation, producing the consistent beer strength that Guinness was
aiming for. The risks of paying too much tax or losing beer to spoilage could
then be eliminated.

This is classic modernization: a method that was previously entirely dependent
on human skill and traditional judgment is replaced by a more efficient and more
consistent method that’s based on scientific methods. And gradually, Gosset’s
methods transformed Guinness, ending with Gosset himself as Head Brewer for
their new London-based brewery. 

## Individualization

At the centre of reflexive modernity is a process of individualization —
essentially that individual people are now required to handle social change and
uncertainty more directly as individuals, and that they have to put time and
effort into this. Where, in previous generations, unemployment (to pick one
example) was a social problem, it is now an individual one. The same goes for
climate change: previous generations could pressure governments to act, now the
shoe is on the other foot, and governments are putting the problems back on us
as an individuals. Worried about climate change? Let’s put a tax on fossil
fuels. 

It looks like “freedom” but this freedom is a sheep in wolf’s clothing, and that
wolf is individualization. Where once we had social systems to support us, now
each one of us is navigating the world alone in our own personal maze of twisty
little passages, all alike. That is individualization.7

One example is the ubiquitous terms and conditions of software. To use, say, an
iPhone, every single one of us needs to agree to several thousand pages of legal
license agreement, with some significant conditions. Most of these are
completely irrelevant (although not always) but they do reflect the way we, as
individuals, are now confronted with these new and complex realities. 

And in fact, the consequences can be much more significant.  Adobe, for example,
recently underlined that individual users could be sued for using old versions
of Photoshop that they bought and paid for. That is because Adobe had the right
to “discontinue” the product, and when it did, the right to use the software
ended. Adobe has yet to enforce this, but the fact that they are preparing to do
so demonstrates the way that individual users are now confronted with handling
the consequences of Adobe’s business decisions -- which they have no control
over or insight into.

Another example is positive psychology. 

In part this is individualization, yes, but also — technology enables
individualization at scale, in a way previous technologies do not. Artificial
intelligence, especially so. 

## Organized irresponsibility

Organized irresponsibility is a class of manufactured risk where people are
*responsible* but not *accountable*. 

Put simply, organized irresponsibility is, as Curran puts it: "when agents are
able to collectively create risks for which each of them are able to avoid
culpability due to the difficulties in attributing specific consequences to
specific actors" (Curran, 2018). "Organized" is important, there has to be at
least an implicit alignment or consensus between multiple participants in the 
system.

One way to think of organized irresponsibility is as a kind of
"anti-Swiss-cheese model", an inverted version of the risk model studied by
James REason and his colleagues at the University of Manchester. Reason's model
describes a system as if it is a stack of slices of Swiss cheese. Each layer has
some holes in it. One layer, obviously, has complete holes, so risks can pass
straight through and turn into actual harms. But if we stack them together, two
or three slices, so that the holes don’t line up, we end up with fewer gaps, and
it is very much less likely that risks turn into harms. 

So far so good. The Swiss cheese model helps us understand how, first, no single
safety system will ever be sufficient; and second, how an assembly of imperfect
safety systems can be far safer than any of them individually. It is widely 
accepted and the basis for most safety-critical systems, including nuclear
power and air transportation safety.

Organized irresponsibility is how a system like this can fail. If individual
agents can punch tiny holes in the layers of cheese, eventually, there'll be a
complete failure -- but, there is no way to pin that down to the behaviour of
any individual agent, because it is always the accumulation of flaws that
creates the risk. And these flaws might be incredibly tiny, so tiny that we 
might not be able to attach consequences to them. 

One example might be masking in a pandemic. If everyone masks, everyone is
protected, but when people choose not to, we cannot attribute any rise in
infections to any one non-masker. Vaccinations work the same, and gun control, 
and almost every other public health issue we might think of. 

How does technology fit into this? 

With risk technologies, risk begins to resemble Gilliam’s movie Brazil, where
responsibility for actions is lost in so many tangled coincidences and
connections, that the whole thing becomes a dark, cosmic joke. Nobody knows who
has the power, for it is distributed -- just like the units in its neural
network. 

## The mechanisms of organized irresponsibility

Organized irresponsibility works by obscuring — intentionally — who is
responsible for a particular outcome. There are several ways that this can
happen, although they are not exclusive, and in any situation aspects of all of
them may be involved.

* **Through complexity.** As a system becomes more complex, identifying causes
  and effects becomes harder. In fact, this is precisely what complexity is.
  Even so, this is far the most common basis for organized irresponsibility.
  Dean Curran formulates this as the *organized irresponsibility principle*:
  “the greater the number of actors involved and the greater complexity between
  causes and the risk’s impacts, the less overall culpability that tends to be
  assigned” (Curran, 2018). In effect, responsibility is diluted across the
  connections, to a point where it becomes meaningless. 

* **Through polarization.** Another characteristic of reflexive modernization is
  that what was non-political becomes political — or, rather, that politics now
  reaches inside business, science, and the other centres of modernization. As
  such, forces like class, identity and political affiliation affect
  attribution. Positive and negative consequences tend to be assessed
  differently depending on political affiliation, and the gap between
  responsibility and accountability widens. Therefore, organized
  irresponsibility may involve misdirecting responsibility to (political) others
  (Galantino, 2022).

* **Through scapegoating.** In the limit, this leads to a different model, where
  instead of those responsible for a hazard, it is those who point them out that
  get the blame. A remarkable instance of this occurred in the Responsible AI
  team at Google, when Timnit Gebru (and consequently other members of her team)
  was fired, essentially for alerting Google to the hazards of large language
  models — even though she and her collaborators were doing so in a constructive
  way, and in the best interests of their employer.

## Malice

One of the key differences between environmental risks and technological ones is
that in the technological world, malice becomes significant. A hurricane may
cause devastation, but it cannot be abused[^MaliceAbuse]. With genetic
engineering there is both a risk of accident and a risk of abuse, but the two
are not the same.

[^MaliceAbuse]: Of course, they can also be exploited — but this is not the same
    thing.

Risk technologies are all technological. Sometimes, they may cause negative
consequences by accident — or, perhaps, through neglect. Self-driving cars are a
good example. When they cause fatal accidents — which they can and do — there is
no intent by the companies concerned. They stand to lose, a lot. It might be
negligence on their part, or sheer bad luck, or some unexpected event, or some
unanticipated behaviour on the part of a driver. These are still devastating —
and in many cases they can be mitigated, especially with the benefit of
hindsight. However, good safety is partly a good process, and we did not get
‘old’ cars to be as safe as they are overnight — it took many hard lessons,
accidents, lawsuits, and redesigns to get there. All in all, though, it isn’t in
the interests of a self-driving car company to intentionally make them harmful.

But malice remains a possibility. Our computers and our phones can be hacked —
so what if self-driving cars could be too? After all, even regular cars can be
used to cause large-scale harm. What if some self-driving cars were hacked to
drive into other cars at speed? This is entirely plausible — it could be
targeted, but there would be a terrifying human cost of millions of self-driving
cars simultaneously switched to drive towards pedestrians rather than away from
them. There is — or should be — a significant pressure on those who build
self-driving cars, to make sure that this cannot happen. Straight environmental
events like floods and wildfires do not have the same risk of malice.
Technological systems, however, often do. 

Of course, what counts as malice isn’t always clear. Many people who abuse
systems may defend what they’re doing (especially to themselves) as some form of
‘greater good’ — they don’t usually believe what they’re doing is actually evil
or wrong. So, for example, if people decide to use artificial intelligence to
sway an election, or to make free art, or to pass an exam, they may believe that
the losses to others are so insignificant (or deserved, or directed at an
out-group that doesn’t merit the same rights) that the benefits are validated.
So the concept of malice isn’t always as clear or as helpful as we might like.

This is why risk is a helpful way to think about it. Malice *exploits* risk. 

## The fear business

Like assets, there is money to be made from risks. In fact, because risks are
produced by modernization, the supply of risks is virtually unlimited — and
that’s the point. 

The simplest and oldest example is insurance. If I own a car, I need insurance
to protect myself from the risks that come with it. There is a lot of money in
this market alone — about a trillion dollars annually. So, insurance companies
provide a service: they provide a buffer between a driver and the risks. 

This works fine in the case of car insurance because cars have been around a
long time, and the risks around them can be straightforwardly estimated. Factors
like age, accident history, where you live, and so on, all factor into a rough
approximate model of risk, which can then be costed. It might not be a perfect
model (if risks were perfectly predictable, they wouldn’t be risks), but it is
good enough that on aggregate, an insurance  market will work, insurance
companies can compete, and for the most part, ‘out of context’ surprises won’t
be so common or so devastating as to disrupt that market.

But what about other kinds of risk, from newer and more far-reaching
technological innovations? What about nuclear power stations, or genetically
modified organisms, or cryptocurrencies, or artificial intelligence? How do
these work?

There are companies that are eager and willing to ‘sell’ risk-based services in
these fields — although for the most part they will never act as a shield for
the technology. Nuclear power stations, pharmaceutical products, genetically
modified organisms, and so on, they are all effectively uninsurable. In fact,
this is why Beck uses the “insurance principle” as an actual metric on risks.

But other companies can still provide services, especially advisory services.
The likes of PwC and KPMG were very early pioneers of this area. For example,
PwC's Responsible AI Toolkit consists entirely of identifying over twenty
different kinds of risks, ranging from performance risks, through security
risks, to reputational risks. Their solution, and their strategy, is all about
mitigating risks — but this is done as a service which doesn’t transfer the
costs of the consequences. 

The sell is clear: if you are a wealthy company and you’re even considering
using AI, you need the likes of PwC on board to help you manage your risks. But
in what sense are they protecting you from those risks? It turns out, what they
are really selling is knowledge, nothing more.

Another company, Arthur, provides a kind of “firewall” for large language models
— in effect a barrier that should reduce the risk of, for example, racist
language that a model might generate. Although, again, as a company, the terms
and conditions reject responsibility. In a way, that is understandable. Unlike
traditional web services, where the risks of attack are now relatively
well-known, AI is a new territory, and the risks are still being discovered.

AI companies themselves also use risk as an essential part of their marketing.
Take Open AI themselves, for example: they actively use fear of AGI to campaign
for regulation. In fact, most large AI companies advocate for regulation — on
their terms. We’ll come back to this issue later. Small companies also use risk,
but usually in a different way — typically to distinguish themselves from their
competitors, large and small.

Governments and regulators also play a key role in risk-based appraisals of
artificial intelligence, although their models and strategies vary considerably
around the globe. For example, the European Union has put risk front and centre
of its AI frameworks, recognizing that applications vary in risk depending on
how they’re used, and the kinds of decisions they make. On the other hand, the
US was a little later to the table, with NIST’s AI risk framework launched in
2023. Although regulation appears to be gradually emerging in both, there are
significant differences: in Europe, it’s building on existing legislation, such
as GDPR, and converging on proposal laws, where the US is (so far) more focused
on providing tools to AI companies, to help them measure and minimize risk.

Finally, the media are also central to the fear discourse in artificial
intelligence. In fact, the media are more central to artificial intelligence
than to most other scientific and technological innovations. Their role is
somewhat double-edged. On the one hand, since 2010, artificial intelligence has
converged on a “Big Science” model (high goals, large companies, virtually
unlimited funding) — and in Big Sciences, historically, scientific publishing
has been edged out by direct media relations and press releases. This is very
apparent. As an example, OpenAI’s Delphi system was never formally published in
a peer reviewed journal at all! It went straight to the pre-print system arXiv,
and to media outlets. The second edge to media engagement in artificial
intelligence is more interesting. They, too, gain from risk — in fact, the more
risky something is, the more newsworthy it is. So, as long as they attract
readers and clicks, stories about the risks and challenges of artificial
intelligence are widespread.

To paraphrase Gordon Gekko, in reflexive modernity, risk is good, risk works. 

## Notes

* footnotes will be placed here. This line is necessary
{:footnotes}
