---
chapter: 10
title: Regulation? What regulation?
layout: chapter
sitemap: false
---

Given the tendency of modern technologies like artificial intelligence to
strengthen power imbalances, and its potential for harm, threaded through risks,
what can we do to control it?

When modernizing creates a new risk, the normal strategy is to *regulate* it.
For example, modern medicine developed drugs which allow diseases to be treated.
In some cases, such as opiates, these treatments themselves create risks, of
addition, for example. And yet, the treatments are still useful under some
circumstances. So, governments created regulations and agencies. Drug treatments
can only be used with a prescription from a licensed (and accountable)
professional, and have to be approved by governmental agencies. Each piece of
regulation acts like a filter, preventing certain types of harms. Although no
single piece acts as a complete protection, but combined together, they become
an effective package to prevent most harms. 

It isn’t that simple, though. Regulation introduces costs, and makes some
people’s lives more difficult. So, inevitably, people and organizations try to
avoid them, in some rather interesting ways. 

First, and simplest, they may simply cheat. Drug companies might try to
misrepresent the data, drugs may be sold direct through an illicit market, or
imported illegally by mail order from another country. These are strictly about
ignoring regulations which do exist, so they’re not necessarily a flaw in the
regulations themselves, although they suggest that enforcement should be
improved. 

Second, they may try to evade the regulations, for example, framing their
products and services in a way that the regulations do not apply. For example,
alternative medicines can be classed as "supplements" even when they comprise
exactly the same kinds of chemicals that recognized medicines do[^Supplement].

[^Supplement]: St John's wort is one example -- its active ingredient,
    hyperforin, affects neurotransmitters in ways very similar to prescription
    antidepressants, yet it is classed as a dietary supplement in the 

Third, they may build a self-regulation system. Self-regulation does have a
place — and an important place. For example, most doctors work under regulation
by a medical board made up of their peers. After all, other doctors may have
more insight into the requirements of regulation than a supposedly neutral third
party outside medicine. But self-regulation by itself can also prevent real
regulation from having any force. It only truly works when governments place
legal *responsibilities* — as well as powers — on regulatory bodies. A good
example was England’s General Medical Council, established in 1858. With minor
changes, this model stood until 2003, when self-regulation was phased out —
primarily because self-regulation had failed to handle a string of high-profile
cases involving hundreds of needless deaths.

Fourth — and most consistent with reflexive modernization — they may attempt to
seize control of the regulations themselves, achieving self-regulation under the
guise of government. The most common form of this is *regulatory capture*, where
by one means or another, the regulator becomes controlled by one of the bodies
it is supposed to be regulating. Today, this is surprisingly common. It tends to
work in two common ways. First, there can be a kind of ‘revolving door’ between
industry and a regulator, with regulators recruited by industry and vice versa,
leading to a much closer relationship between them than with those they are
supposed to be protecting by regulation. A second mechanism for regulatory
capture is through academia: if industry deploys significant funding to
universities and academics, they can shape public discussion in ways that
benefit their benefactors. 

## Privacy

One of the regulated rights that has most affected artificial intelligence has
been the right of privacy. Particularly in the European Union, this has
significantly the shaped the way data is collected and used, through a series of
progressively demanding regulations. However, given the cross-jurisdictional
nature of the digital world, its influence has been global, and the EU’s
regulation has established a model that has influenced a significant part of the
world.

Put simply, the EU’s *General Data Protection Regulation*, or GDPR, does the
following:

* Regulates the way people’s personal data is collected and processed by
  organizations in Europe
* Regulates the way any EU resident’s data is collected and processed, globally

Under this regulation, for example, a US-based company cannot simply keep and
use the personal data of an European resident without their consent, or without
some overriding reason. In fact, even moving a European resident’s data to a
data centre in the United States — where it may be vulnerable to access and use
by government bodies (under, for example, the US Patriot Act) without the rights
that citizenship brings — is also open to potentially punitive damages. 

<div markdown="1" class="box">

## Privacy by design

The most common approach to developing technologies that respect privacy is Ann Cavoukian’s *Privacy by Design*. This was originally developed when Cavoukian was Ontario’s Privacy Commissioner, but has since been embraced internationally, especially by the European Union. Privacy by design is based on seven “foundational principles” — all of which relate closely to the way data is used in artificial intelligence.

1. *Proactive not reactive; preventive not remedial* -- where possible, look for
   -- and plan for -- risks in advance, rather than waiting for them to happen. 
2. *Privacy as the default setting* -- don’t collect data by default and then
   expect people to ‘opt out’: respect privacy and then encourage people to opt
   in by making it worthwhile.
3. *Privacy embedded into design* -- privacy should be part of the design from
   the very beginning.
4. *Full functionality – positive-sum, not zero-sum* -- don’t think of invasion
   of privacy as a necessary evil that pitches developers against users; design
   so that it is in a user’s interest to actively share data.
5. *End-to-end security* -- full lifecycle protection -- security is not an
   add-on, it needs to be designed in from the very start; this covers data
   collection, storage, processing, and disposal. 
6. *Visibility and transparency* -- keep it open
7. *Respect for user privacy* -- keep it user-centric

</div>

Interestingly, many of the (good) principles of privacy by design have fallen into
disuse in the decade since. There are several reasons for this, largely linked to
the modernization in the technology industry. In particular, most artificial 
intelligence technology (which, after all, depends on access to copious amounts 
of data) creates a pressure to collect data by default, with opt outs only when 
required by regulators or valuable customers.

## Notes

* footnotes will be placed here. This line is necessary
{:footnotes}
