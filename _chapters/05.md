---
chapter: 5
title: The strange case of Open AI
layout: chapter
sitemap: false
---

I could write a whole book on Open AI and its gradual evolution from an
apparently humanitarian effort to apply technology for the betterment of
society, into the organization leading the charge of artificial intelligence as
a socially transformative technology.

One particularly intriguing part of Open AI is its valuation. As with a range of
modern technology companies, the valuation of Open AI has almost nothing to do
with its sales. Instead, its valuation is based almost entirely on its
*potential* for future market domination.[^Tesla] With this kind of technology
company, it is increasingly common for the bottom line to become almost
irrelevant compared to, say, "fear of missing out". The phrasing is
illuminating: the risk of other people succeeding forces people into
participating.

Open AI is far from the only company like this. Amazon and Tesla both took over
a decade to generate a profit. High growth industries do tend to generate higher
valuations -- again, this backs up our thesis that a positive risk (a future
potential, if you like) can dominate a valuation. 

A second illuminating aspect of Open AI is its response to regulation. It has
been a vocal advocate for self-regulation, and against government intervention
-- except where that government intervention is to formally recognize
self-regulation, which doesn't count. It has led this strand of its activity in
a novel way. 

The traditional approach corporations take to regulation is: lobbying
governments to minimize its impact, or when regulation is inevitable, to try to
capture the regulator. Lobbying is the first strategy, persuading legislators
not to regulate, and supporting the elections of those friendly to the industry.
Later, when (and if) there is a relatively powerful regulatory body in place,
the second strategy comes into play, trying to take control of the regulator,
lobbying to appoint former employees, friendly lawyers, or other third parties.
With them in place, even if the regulations are established, their impact on the
industry can be minimized. 

Open AI did do lobbying, but it also took a new third approach: acting as if it
was an equal of governments, able to negotiate with them as a new kind of
corporation -- one outside the sphere of influence of governments entirely. The
actual discussions were not the important part: this was performance designed,
in large measure, to establish the technology companies as the only actors
capable of making decisions about the technology -- a kind of self-regulation on
steroids. 

[^Tesla]: Another good example is Tesla, which, relative to its actual sales, is
    valued very differently to its competitors. One could argue: that is because
    it's not a car company but a technology company. This is a reasonable
    analysis, but it is incomplete: *why would a technology company have such a
    different valuation?* The short answer to this is: because technology
    companies are, generally speaking, valued less on their current sales than on their
    future potential.

Open AI was founded in ???? by ????. Its initial stated mission in 2016 was: "to
build safe AI, and ensure AI's benefits are as widely and evenly distributed as
possible".

### The strange case of Open AI

As with Stevenson's Dr Jekyll, there are two sides to Open AI, two sides in
conflict. There is the respectable and outwardly ethical company, investing in
social policies such as a universal basic income. And there is the malevolent
and barbaric company, crawling the internet to appropriate all the data it can
in the name of "fair use", only to transform it into services which compete
directly with those who created that data in the first place.

The similarity does not end there. After all, Jekyll's motive for creating his
serum was to eliminate his dark side, his evil impulses. Of course, it did not
work out well for Jekyll either. What began as occasional lapses into malice
gradually increased in frequency, with a slow loss of control until, eventually,
only Hyde was left to face the consequences of his actions.

When it was published in the Victorian era, Stevenson's tale explored the way
respectability and morality worked in Victorian society. It reflected the 
modernity of that age, science and medicine used for human betterment.

In many ways, we live today in a corporate analogue of that time. Science and
medicine have progressed, but they are no longer the province if individual
researchers: today, only universities, corporations, and other institutions have
the resources to deliver. The thin line between good and evil in individuals has
become an equally thin line between the positive and negative impacts of
corporations[^Google]. And just as in Victorian society, there is a pressure on
corporations to maintain a respectable image, a pressure for social
responsibility. 

In this corporate retelling of *The strange case...*, what, then, is the serum?
Maybe it is artificial intelligence itself. It is a technology and a product of
science, and it is intended (at least in part) to separate the gold of reason
from the dross of ignorance. And yet, it fails. The technology itself has an
addictive quality that, while seeming to strengthen reason, actually does the
opposite. Jekyll's humanity is itself destroyed, not by his dark side, but by
the gradual erasure of his light side. That is the corporate risk of artificial
intelligence: rather than buttressing us against our lapses to the asinine, it
undermines what's left of our reason.

A second duality that can give us an illuminating perspective on corporate AI is
Ulrich Beck's original one, between 'goods' and 'bads', between benefits and
risks. Despite Beck's punning, he's firm that risks are not intrinsically bad. A
balanced state involves some risk -- they are necessary to a healthy state.
Attempting to eliminate risk can, paradoxically, create even more risk. 

[^Google]: An even more striking version might be Google, starting out with
    "don't be evil", yet within a couple of decades, designing and building
    battlefield robots, and finally repudiating their original motto. And yet,
    unlike Open AI, Google couldn't be Jekyll -- despite its mission statement,
    Google was never striving to be good. 

## Risky business

As we might expect given how today's technology exemplifies reflexive modernity
and the modernization of industrial society, companies like Open AI make risk a
central part of their services. All technology companies embed large limitation
sections and indemnities and disclaimers into their terms and conditions, all
with the express purpose of minimizing their risk. Open AI is no exception in
this respect -- but it does make sense, nobody writing a commercial email
program would want to get sued if that program was accidentally implicated in a
chain of events that led to something catastrophic.



## Notes

* footnotes will be placed here. This line is necessary
{:footnotes}
