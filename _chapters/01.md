---
chapter: 1
title: Introduction
layout: chapter
sitemap: false
---

{:.mark}
This is a weak introduction, and it needs a complete rewrite. 

## Risk machines

This is a story about technology, about how it made -- and broke -- the modern
world that we live in today. 

But also, in some ways, this book is not about technology at all, but about
society and how it is shaped by machines. Or, strictly, not by machines
themselves, but by the forces those machines embody. 

I will be honest: I don’t know what kind of society we are creating. And I don’t
believe anyone else knows either. Some claim that they do, but their visions
resemble science fiction more than reality. The flavour of science fiction they
anticipate varies widely according to the individual, from the fantastical
techno-utopianism of Musk to the dystopianism of X. What they never come clean
about is why their vision is any more likely than anybody else’s — there’s never
a theory behind that.

So although I don’t want to paint a picture of the future, I do want to explore
the forces that are influencing its trajectory — and in particular, the process
of modernization, the force that created machines, and which, through machines,
is transforming the world we live in. 

## The technology industry

Most of the changes that we see are associated with a loosely organized
coalition of large and small technology companies, with a distinctive identity
as part of a technology industry sector. The larger and more famous companies
are household names: the likes of IBM, Microsoft, Google, Apple, Tesla, eBay,
and Amazon, but they are surrounded by a plethora of smaller ones. 

If we take a step back, we can see that all these companies have one thing in
common: they are all applying technology in the name of progress, to modernize
things, whether it is industry, government, society, or our leisure time. IBM
led this model for many years, helping industry automate finances, payrolls, and
inventory management, across almost every business sector you can imagine. But
with the advent of the personal computer, and then subsequently mobile
computing, other companies continued the process, transforming retail,
advertising, transport, driving, and healthcare. In fact, if you can think of a
transformative innovation, the chances are, some technology company will be
behind it.

This is not necessarily a bad thing. Sometimes, innovation genuinely does make
for a better world -- it does happen. The innovations aren't simply thrown out
at random, they are consistently planned on the basis that they make a
difference, to someone. The big question is, of course, who do they make a
difference to? These are companies -- their *raison d'etre* is to make money,
and they aren't in business from the kindness of their hearts. 

In all cases, the basic process is the same: modernization. Let's pick Amazon to
demonstrate the point. In the early days of retail, buying things involved going
to a physical store, picking what you want off a shelf, paying for it, and
taking it away with you. This meant that the range of products you could buy 
was physically limited by the space in the store. 

One solution to this was a marketplace: instead of having one retailer, you
could host dozens of them in a single space. This is very convenient for a
buyer, they can wander around and choose from the full range. Another innovation
that sets out to solve the range issue (and another surprisingly old one) was
the introduction of catalogues and mail order. Again, you could choose what you
wanted from a much larger range, this time without physically going to the store
itself. Catalogue stores enabled a kind of industrial scale for shopping that
traditional stores don't: they enable the same kind of economies of scale that
production lines bring to manufacturing -- and along with the economies of scale
come reductions in costs that make these stores able to offer cheaper prices.

The big impact of technology came into retail through the introduction of the
internet. Telephones and other communication tools had, obviously, helped the
catalogue companies, but the printed catalogue itself became a limiting factor.
With the web, the catalogue didn't need to be printed at all, and it could
change as often as it needed to. So, while a few early internet companies
experimented with "internet malls" -- a business model that was perfected years
later by Shopify -- Amazon used books as a way to establish a new kind of
technology-enhanced shopping system that very effectively combined markets and
catalogue stores.

Books were a perfect jumping off point, because catalogues don't really work.
New books are coming out every single day, so the mail order book companies that
did exist (and there were quite a few) tended to have a limited range,
especially of newly-published items. Amazon could create an instant catalogue,
and update it whenever they wanted, and with a few clicks people could buy the
latest releases and have it shipped to them. 

eBay evolved in a different way. One of the advantages of a marketplace business
is that you don't actually need to sell products at all. You run a virtual
space, and bring together sellers and buyers. By taking a small amount from
sellers, perhaps by taking a small cut from payment transactions, you can make a
tidy profit without having to run warehouses, supply chains, or all the other
inconvenient trappings of an actual retailer like Amazon. But even for the likes
of eBay, networking technology enables people to buy and to sell any time that
they choose.

In both cases, the technology company is doing two things. First, they are 
radically transforming an existing system, especially in a way that enables
a significant increase in economies of scale, making them more competitive
than other options. And second, they are injecting themselves into a new kind
of business. Amazon and eBay are not only retail outlets, they are technology
companies too. They blur the line between innovations and the solutions they
are setting out to replace.

## Artificial intelligence

It would be easy to focus too much on artificial intelligence as ‘the’
technology of risk, but that would be misleading. There are many different ways
that technology today shapes the social flow of risk, and much of it has nothing
to do with artificial intelligence. Cryptocurrencies introduce some fascinating
risks — through their determination that transactions cannot be reversed, even
if they are fundamentally fraudulent. Cellular phones enable our risks to follow
us around our daily lives, biotechnology means that risks can cross into the
realm of the biological, social media injects risks into our daily and intimate
social interactions. None of these are necessarily dependent on artificial
intelligence at all. However, what is special about artificial intelligence is
that it enables bespoke risks to be manufactured at scale, but still on a
personalized basis. This is new and unique. It is the ability of this specific
technology to tailor its behaviour to each person’s individual circumstances —
and to do so at global scale — that makes it such an important part of this new
phase of modernization. 

What makes artificial intelligence unique? 

If you ask ten artificial intelligence developers that question, you’ll probably
get eleven different answers. Some will emphasize its speed, others is ability
to learn from past examples, others its ability to (pretend to) creativity.  

Artificial intelligence plays a role similar to that mechanization in the early
Industrial Revolution. At that time, mechanization allowed human skills to be
captured and deployed at scale. This reduced the need for people with those
uniquely human skills, and enabled their previous employers to increase their
efficiency while reducing their costs. Commercially and economically, of course,
this makes perfect sense.

It also clarifies the relationship between economics and automation. Jobs aren’t
automated at random. The ones that are automated tend to be (a) simple enough to
automate, (b) costly enough that they are worth automating, and (c) in
sufficient demand that it’s worth investing in automation. All three factors
play a role. They aren’t precisely conditions, in the sense that they’re all
parts of a cost-benefit analysis, so simple tasks are more open to automation
anyway, even if they’re not as costly.

Artificial intelligence has always had a relationship with automation. This was
explicit, for example, even in 1973’s Lighthill Report, which was actually very
positive about the impacts that AI had already had on automation. That field of
work was expected to continue. 

What artificial intelligence brought was a change in what could be automated. Up
to that point, it had generally been routine manual skilled work that had been
automated. Tasks like cotton spinning and textile weaving were among the first,
but as the 19th century turned to the 20th, that gradually expanded to
machining, assembly, agriculture, mining, and so on. By the 1980s,
industrialization had reached a point that the need for skilled employees was
very significantly reduced, and entire economies began to move away from
manufacturing and towards services.

So how does automation work in services? Perhaps the first widespread innovation
of that kind was the banking automated teller machines, which started to achieve
widespread impacts in the late 1970s. This was among the first machines that
took a very specific human task — dispensing cash from bank accounts — and
automated it. In doing so, it created a whole set of new risks. Indeed, plastic
cards for banking demonstrated exactly the kind of risk movement that Ulrich
Beck describes. Personal information number (PIN) authorization allows the banks
to say that if a card is used fraudulently, then it’s the customer’s
responsibility to prove that it was fraudulent, where up to that point, the
banks had been liable. A new risk is created (people stealing the PINs and using
cards fraudulently) and that risk flows away from the large and powerful bank to
the individual customer.

Another interesting precursor to the automation of services is credit scoring.
This time, what is automated is not a simple transfer of money from an
electronic account to physical cash. This time, it’s a decision! At around the
time time the ATM was going mainstream, computing technology also consolidated
thousands of small credit bureaus, each with their own paper-based filing
systems, into a few large organizations with information retrieval technologies.
Today’s credit scores are built on that, and essentially provide a simple code
which quantifies someone’s financial risk. If you have a sufficiently good
score, an automated system may even offer loans directly, without a human
review. 

Now you might think that this is simply statistics, but trust me: credit scoring
grew directly out of artificial intelligence[^Disclaimer]. In fact, credit scoring is an
excellent early example of machine learning. 

[^Disclaimer]:
    I have personally worked with artificial intelligence researchers who
    developed credit scoring.

But the humble ATM was just the beginning. What artificial intelligence allows
is far more powerful decision-making. Instead of managing credit, today,
artificial intelligence may be used in decisions that affect hiring, career
progression, healthcare, education.

Perhaps more interesting is that credit scoring actually grew out of regulation.
It was also expensive, both to develop and to manage. Although the artificial
intelligence techniques of the time (which were, notably, statistics and expert
systems) were deeply involved, expert systems in particular are both expensive
and complex to build. They’re expensive because they require costly technical
people and costly experts to spend time together, not doing their normal jobs,
but analyzing the decisions the experts make in sufficient that they can be
automated. In effect, they involve the kinds of time and motion study used in
early 20th century scientific management, but on tasks that are an order of
magnitude more complicated. They’re complex because they need to be good enough
to match up to performance of the human expertise, which is not an easy goal to
achieve.

The modern changes in artificial intelligence, and in particular the advent of
more powerful approaches to machine learning, radically change the economics
compared to the era of expert systems. In particular, it doesn’t demand large
amounts of expert time. Instead, the algorithms can work directly with the same
information the experts receive, and the decisions that they make. All that data
can be cleaned and used to build a predictive model for a fraction of the cost
of an expert system, and usually greater reliability in the process. That’s in
the nature of machine learning: by and large, unusual cases are less surprising
when a system has been trained on a relatively large amount of data, compared to
when rules are assembled by hand by experts reviewing a relatively small amount
of data in depth.

However, all the issues raised by credit scoring algorithms in the 1980s haven’t
gone away. Essentially the same issues — bias, predictability, potential for
harms such as redlining — these all still exist.

Credit scoring also exemplifies modernization. Before the new systems were
introduced, credit scoring was based on human judgement. It was fragmented
across hundreds, maybe thousands, of credit offices. And so was the data, which
was stored on paper in good old-fashioned filing cabinets. During the process,
technology integrated the data, computerized it for access and analysis, and
developed new methods of identifying which features were effective for
prediction. The result, a sort of hybrid of statistical analysis and human
expert rules of thumb, could scale up to handle hundreds of decisions a second.
And, by building on science, those decisions themselves were more consistent and
less likely to run foul of government regulations and legal challenges.

## Notes

* footnotes will be placed here. This line is necessary
{:footnotes}
