---
chapter: 6
title: Perceptions of risk
layout: chapter
sitemap: false
---

{: .mark} 
**Note.** A few things to revise here. We need to lead more with risk chains, a
new concept kind of like a value chain, but flipped as Beck does. This is an
important and useful concept, essentially underpinning the way foundation models
work in business. And similarly, the role of artificial intelligence can start
further down, as we want to make this more agnostic about how people perceive
risks. Many of the exact same arguments can be made for, e.g., 3D printing, mRNA
vaccines, cryptography, and so on.

## Modernization with artificial intelligence

Artificial intelligence is not unique as a computer-based technology that
enables automation and improved efficiency, but by harnessing human expertise,
it creates new opportunities beyond those of more conventional technology. 

In the European Union, the AI Act nicely classifies applications by the levels
of harm that they introduce. Importantly, this is done more by application area
and the kinds of decisions made about people, than it is by technology. This
makes sense: the exact same algorithm can, for example, classify people by
behaviour (social scoring, unacceptable risk), recommend training for workers
(education, high risk), or prioritize emails (low risk).  In other words, risk
is not a property of an algorithm, but of an application using that algorithm.
Table 1 below shows some of the recommended levels of risk for specific
application types.

| Level of risk | Application area                            | Examples            |
|---------------|---------------------------------------------|---------------------|
| Unacceptable  | Manipulation of people or vulnerable groups | Cambridge Analytica |
| Unacceptable  | Social scoring                              | Classifying people by behaviour or social status |
| Unacceptable  | Real time remote biometric identification   | Facial recognition systems |
| High          | Biometric identification and categorisation | Clearview AI        |
| High          | Operation of critical infrastructure        |                     |
| High          | Education and training                      |                     |
| High          | Worker management                           |                     |
| High          | Access to essential services                |                     |
| High          | Law enforcement                             | Clearview AI        |
| High          | Border control management                   |                     |
| High          | Application of the law                      |                     |
| Limited       | Image generation                            | Midjourney, DALL-E, Stable Diffusion |
| Variable      | Text generation[^GenerativeAI]              | ChatGPT, Bing       |
{:.w-100.table}

[^GenerativeAI]: Generative AI is new enough that the levels of risk are not yet
    well known. Some applications are extremely high risk (e.g., personalized
    and targeted fraud) where others are likely low risk. This exemplifies why
    attaching the level of risk to a technology rather than an application is
    problematic.

Setting aside the application areas that are unacceptably risky for now, the
others are clear modernization. For example, border control used to be done
entirely by people. Electronic passport controls, with biometric photo scanning,
allow a few border agents to handle a large number of people more efficiently,
by allowing the easy cases to proceed quickly and easily. The down-side is that
automatic decisions may be disproportionately worse for certain ethnic groups,
for example — and that is why it is a high risk application area. Similar for
education: in theory, automated assessment and grading improves efficiency and
assists teachers in a relatively onerous and burdensome aspect of their work. In
practice, if it isn’t scrupulously fair, the technology can have consequences
for people’s entire future lives and careers. So, again, it’s high risk.

These generally cover areas where people are *recipients* of decisions made by
AI systems. That is not the only kind of risk these systems introduce. For
example, DALL-E, as a generative AI system that produces images, while it may be
‘low risk’ for the people who ask for images, it is substantially higher risk
for the creators whose images form part of its training data, and whose income
may be jeopardized by these applications. Risks are rarely dealt out
even-handedly by AI systems: as Beck noted, where wealth tends to accumulate at
the top, risks tend to accumulate at the bottom. This uneven distribution of
risks is where our new risk technologies depart from traditional modernization.

Secondly, this table is a good map of many known risks, but many risks may
remain unknown, at least at first. Generative AI provides several examples of
this: superficially, it may seem that the risks of text generation via ChatGPT
are relatively low[^LowRisk], and yet its integration into educational
assessment has already proven able to make much of educational assessment less
fair — especially for those who do not use the system. This is a nice example of
Beck’s ‘latent side effects’ — and like many side effects, they may be tangled
up as perverse incentives. In this case, even those who disapprove of ChatGPT
may be forced to use it, simply because the alternative is a marked disadvantage
compared to their peers. 

[^LowRisk]: Open AI has explicitly asserted this, and argued that their models
    should not require heavy regulation because they are not an application
    company. In terms of the EU framing, this makes sense. However, seen in the
    context of a risk chain, it can also be a way for large technology
    corporations to evade regulation entirely, by downloading risks onto
    application vendors.

## Foundation models

One of the challenges to risk assessments in artificial intelligence has been
the emergence of large, pre-trained, and domain-independent models — so-called
foundation models. This means that a company who wants to build an application
in a risky area, for example, educational assessment, might not build a large
training set for themselves and start from a blank slate. Instead, they use a
foundation model and fine-tune it to meet their needs.

In many ways this is a wonderful innovation — and it does go a long way towards
addressing some of the real challenges of scaling artificial intelligence.
Because fine-tuning requires a tiny fraction of the compute power of the
original model, obviously the overall energy demand of, say, 20 companies using
the same foundation model and fine-tuning it, will be very much less than if
they all set out to build a large model themselves from scratch. The same goes
for the demand for human labour and for data. 

In one sense, this is not unlike a software library like ImageMagick.
Application developers can use the library for all sorts of different purposes,
thanks to its flexible API. It obviously saves application developers from
having to re-do all sorts of boring image processing and manipulation tasks,
which are likely a complete distraction from their plans. But also, it sheds
liability. As the ImageMagick license says: “You are solely responsible for
determining the appropriateness of using or redistributing the Work and assume
any risks” This is not at all unusual, particularly for a simple and transparent
component like ImageMagick. For something as complex and opaque as ChatGPT, it's
another question. But this same approach reflects the way Open AI has pitched
its use of foundation models with the EU, for example: risks should not be
attached to foundation models, only to applications that use them.

How this will work out in practice remains to be seen. For our educational
assessment company, for example, fairness is crucial. If they fail to handle men
and women consistently, they will be liable for heavy punitive damages. However,
gender fairness may be fundamentally flawed in the foundation model itself,
before it is fine-tuned. Yes, there is definitely some responsibility on an
application developer; they need to test methodically, and theirs is the duty to
sign off on any deployment. But while the obligation is firmly on the
educational assessment company. Open AI does have some responsibility
(especially with respect to privacy, retaining data, and so on), although it
uses an “as is” statement in its terms of use to limit its impact. And most
countries do have commercial rules that imply that sold goods need to be of
merchantable quality. However, there aren’t many past cases to rely on, and how
any of this might pan out in court is likely anyone’s guess. 

Taken at face value, though, an API like GPT-4 offloads risks onto its users,
typically the application developers who will attract both the liabilities and
the enforcements from the likes of the EU. This makes them an extremely
attractive proposition, both legally and financially, for the likes of Open AI.
Not only can they deploy into a wide variety of different markets — admittedly
through partners rather than directly. What risks they do have, relating to
whether their models are fit for purpose, are spread across the markers. But
beyond that, Open AI assumes none of the legal liabilities of deploying an
application in any of those markets. Instead, they simply collect the revenue.
This is very much in line with Beck’s theory: by default wealth flows up, risks
flow down. 

So, foundation models are very appealing to investors, but to be attractive,
they also need an ecosystem of application developers willing to assume the
residual liabilities of deploying in potentially sensitive areas. For Open AI,
this is where Altman’s links with Y Combinator come in. Above all, Y Combinator
is a pipeline of high-risk application developers. The high-risk element comes
from their status as relatively early-stage startups. Although there will be
some investment in them, it will not be a massive amount — although Y Combinator
itself (and its network of mentors and advisors) will also take a stake. 

Other foundation models, like Google’s Bard, are likely to follow a similar
trajectory: claiming that as something large enough to be independent of any
single application, they should be considered low-risk, while assembling a
network of channel partners to enable deployment in application areas. 	

In effect, a foundation model is more a go-to-market strategy than a piece of
technology. It is a way to approach bringing a large artificial intelligence
model out in a way that insulates investors from risks. This is not a trivial
issue for investors, either. All the major artificial intelligence companies
have had public relations crises around their earlier efforts in the field,
ranging from Microsoft’s Tay, Open AI’s Delphi, Amazon’s hiring crisis, and
Google’s initial launch of Bard. Bard alone wiped $100 billion from Google’s
market value, in a single day. They — and their investors and shareholders —
need ways to reduce their exposure to risks in these large deployments. 

## Cognition of risk

One thing humans are terrible at is actually grasping the probabilities of a
risky event. We aren’t calculating machines, capable of inferring that a
probability of 0.05 means that out of every 20 tries, one will succeed. Instead,
we are a mess of dirty heuristics that help us function in a risky world. These
heuristics are surprisingly effective, most of the time, but because they are
heuristics, when they are wrong, they tend to be wrong in systematic ways.

This has been extremely well-studied in psychology, not least by the two
(separate) Nobel prize-winners, Herb Simon (also a pioneer in artificial
intelligence, working with Allen Newell), and Daniel Kahneman (for his work with
Amos Tversky). Neither won the prize specifically for psychology but in
economics, both groups separately tackling the long-standing theory that humans
were — or should be — rational decision-makers, *Homo economicus* — or ‘economic
man’. Perhaps unsurprisingly, the concept of *homo economicus* was a central part
of the modernization of economics — it brought science into the field, and
allowed precise mathematical predictions of how decisions within an economy
might work. 

Enter psychology. Simon’s work was perhaps more fascinatingly informed by
artificial intelligence. He had experience building computer models of how
people make decisions, and that experience showed that people simply cannot be
perfectly rational. It’s impossible, for example, because we cannot assess all
possible courses of action[^FrameProblem]. And secondly, Tversky and Kahneman
designed a range of elegant experiments that convincing showed people do not
base their decisions of rationality anyway. Essentially, they took Simon’s point
and experimentally developed a theory of how people actually do make decisions.
Tversky and Kahneman’s theory, prospect theory, suggests that a decision is made
in two steps: first, people identify possible actions, rank them heuristically,
and look at the advantages and disadvantages as gains and losses. Only then do
people evaluate the actions for utility.

[^FrameProblem]: Interestingly, the “impossible” relates back to McCarthy &
    Hayes ‘frame problem’ — it is impossible to list and evaluate all possible
    actions, and, therefore, to be perfectly rational when making a decision.
    This is not only impossibility for humans -- it is a philosophical challenge
    that would apply just as strongly to any AI. Therefore, this argument
    implies that even super-intelligent AGIs cannot be perfectly rational
    either, even in principle.

But there’s a lot going on under the hood here. First, losses and gains are not
symmetric. In decision-making, losses count for more than gains — another bias
known as loss-aversion. 

Secondly, framing is a key part of Kahneman and Tversky’s theory. Basically, the
way an issue is presented dramatically changes the way that people respond to
it. In part, this is the trite “a problem is an opportunity in disguise”, but
the effect is a real one — if a decision as framed as an opportunity, people
make very different decisions compared to when it is framed as a problem. This
is even visible in the artificial intelligence AGI debate — if the discussion is
framed as an entirely negative risk, people respond very differently compared to
a more balanced debate over the advantages and disadvantages of the technology.
The same goes for other technological risks like genetic engineering and nuclear
power.

| Bias                  | Effect                                                 |
| --------------------- | ------------------------------------------------------ |
| Loss aversion         | People over-weigh possible losses compared to possible gains; people over-weigh low probability events and under-weigh high probability events |
| Myopic loss aversion  | People over-weigh short-term losses and gains, and under-weigh long-term losses and gains |
| Risk aversion         |                                                        |
{:.w-100.table}

Over and above the basic mechanics of decision-making, another of Tversky and
Kahneman’s effects that influences decisions is the availability heuristic. Put
simply, we are strongly influenced by immediate examples that come to mind. If
we can remember something, it’s important. This is a highly significant driver
of anti-vaccine sentiments, for example. Any sufficiently vivid anecdote about a
positive benefit or an adverse reaction, if it comes to mind when we are making
a decision about whether to get vaccinated or not, will shape the way we make
our decisions. However, this too is shaped by loss aversion, so negative
anecdotes tend to be more influential than positive ones.

The availability heuristic is particularly important when it comes to risks. If
our news media is filled with stories about crime, we believe crime to be more
common than it actually is, and we act accordingly. We may decide not to go out,
or to install an alarm. It doesn’t matter that we may be overestimating the
actual risk substantially — that’s the availability heuristic at work. Obviously
this means that the news media, and increasingly social media, shape the way
that we react to risks. We don’t ‘think it through’ or use rational
decision-making. We decide what is important by what comes first to
mind.Unfortunately, this means that it is possible to engineer media so as to
falsely engineer the perception of risk, through what is known as an
availability cascade.

The availability heuristic is extremely powerful. Much of the recent rise in
anti-vaccination activism comes from it, in two different ways. First,
vaccination has been so successful that an entire generation is completely
unaware of the risks of disability or death from previously common diseases like
polio and measles. The benefits of vaccination are therefore discounted.
Secondly, though, stories on social media about the harms of vaccines — even
when the stories are entirely false — drive people’s fears and their reactions
to vaccination. 

## Availability cascades

One of the more intriguing aspects of the current discussions about the
existential risks of artificial intelligence has been watching an availability
cascade happen in real time. An availability cascade (see: Kuran & Sunstein,
1999) is a special kind of ”going viral”, where what goes viral is public
perception of an immediate and serious risk. It’s a social feedback loop where
individuals, the media, and interest groups of various kinds together create a
dynamic that massively amplifies awareness of a risk. The difference is that
what goes viral is availability in the sense of the availability heuristic, a
cognitive bias that means what is at the front of our minds is more likely to be
important that everything else.

In effect: intense, frequent public discussion of an issue — any issue — creates
an imperative that (to use David Allen Green’s phrase) ”something must be done”,
and drives a demand for policy action as “an assertion of political virility“.

That this drives the ‘existential risk’ narratives inside artificial
intelligence shouldn’t be a total surprise — availability cascades have been
part of the literature around the psychology of risk for a few decades now,
discussed in detail by Kuran and Sunstein (who first described them) and in
Kahneman’s (2011) Thinking Fast And Slow.

The "existential risk of AI: meets almost all of the Kuran & Sunstein's
“aggravating factors” (see Table 1) of an availability cascade (i.e.,
uncontrollable new technology, heavy media coverage, human-generated
irreversible impact on future generations, and poorly understood mechanisms).
This is a perfectly crafted set up for an availability cascade.

| **Risk Traits** | **Aggravating** | **Mitigating** |
| :----- | :----- | :----- |
| **Familiarity** | New | Old |
| **Personal control** | Uncontrollable | Controllable |
| **Voluntariness** | Involuntary | Voluntary |
| **Media attention** | Heavy media coverage | No media coverage |
| **Equity** | Evenly distributed | Unevenly distributed |
| **Impact on children** | Children at special risk | Children not at risk |
| **Impact on future generations** | Future generations at risk | Future generations not at risk |
| **Reversibility** | Irreversible | Reversible |
| **Identifiability of victims** | Victims known | Victims unknown |
| **Accompanying benefits** | Benefits clear | Benefits invisible |
| **Source** | Human-generated | Natural origins |
| **Trust in relevant institutions** | Low trust in institutions | High trust in institutions |
| **Immediacy of adverse effects** | Adverse effects immediate | Adverse effects delayed |
| **Understanding** | Mechanisms poorly understood | Mechanisms well understood |
| **Precedents** | History of accidents | No past accidents |
{:.w-100.table}

Table 1. Acceptability effects in availability cascades (from Kuran & Sunstein)
{:.caption}

Now look through Table 1 again, but through the lens of AI risks. How does it
score for each trait, as an aggravating factor or a mitigating one? In just a
few lines (new, complex, uncontrollable technology, with heavy media coverage)
the setup for a powerful availability cascade is perfectly in place.

Kuran & Sunstein describe two separate dimensions to an availability cascade: an
informational dimension and a reputational dimension. The informational
dimension is relatively straightforward: people believe there’s an existential
risk because other people people believe there's an existential risk. It’s a
simple matter of viral spread, with the usual mechanisms of media and social
media dissemination driving the amplification.

The reputational dimension is more interesting. As Kuran & Sunstein argue, some
agents are availability entrepreneurs, activists who intentionally trigger
availability cascades for their own benefit, which may be social or reputational
as well as financial. Geoffrey Hinton and Sam Altman both qualify, but there are
many others too. Not all agents are individuals, there may be companies (e.g.,
Open AI, PwC) and NGOs (e.g., the Center for AI Safety and the Future Of Life
Institute) as well as politicians (e.g, Ursula von der Leyen, Narendra Modi) and
— especially — normally trustworthy media organizations like The New York Times,
Time, and the BBC. All stand to gain by mutually reinforcing the risk and using
each others’ reputations to bolster their own impact. And it works, not least
because risks of negative events get much more attention than risks of positive
ones (Kahneman, 2011).

In effect, agents participate for many different reasons (which are not mutually
exclusive):

1. They may genuinely believe there is a risk.
2. They may believe there is a risk because other people they trust believe
   there’s a risk.
3. They may participate because it directly enhances their, or their group’s,
   credibility with other, high-status agents
4. Participation promotes group goals, such as possible future funding.
5. Participation may be a marker of their social identity.

In addition, those who don’t participate receive negative feedback rather than
positive feedback. In this case, they include people who question the
existential risk of AI, or raise concerns about the long-standing other risks.
These agents are socially penalized. This process is clear in the excessive
criticism of those who do not ’buy into’ the existential risk frame -- often
because they see other, more immediate risks, as more worthy of attention as
well as regulation.


## Notes

* footnotes will be placed here. This line is necessary
{:footnotes}
