---
chapter: 12
title: Imagined futures
layout: chapter
sitemap: false
---

## Slow technology

There is one movement that is pushing back on modernization, and doing so in an
intriguing way. That is the “slow technology” movement. 

Slow technology is the opposite of solution-oriented. It is about the
experience, not the goal. It is *envelopement*, not development. Successful and
effective technology systems -- artificial intelligence or not -- can’t simply
be “deployed”. Instead, there’s what Madeleine Clare Elish calls “repair”,
additional work needed to synchronize the technology with the culture, work
practices, and power dynamics. This does not happen for free — and in fact it is
largely hidden work that is entirely crucial to the success of the technology.
Slow technology is about taking the time needed to understand, respect, and
enclosing the technology within that cultural context. It is designed not to be
disruptive, instead, it’s about designing to improve, and deliberate over, our
experiences.

There is a psychological aspect to this, too. Daniel Kahneman argues that we act
as if there are two competing systems inside us: System 1, an instinctive system
that acts reflexively; and System 2, a deliberative system that acts
reflectively. The problem is, faced with events in the real world, System 1 is
faster and often wins out. For example, when we are trying to decide if
something is true or not, System 1 often leaps to the conclusion that it is
true, and only when (and if) System 2 gets involved, may we reconsider and come
to a more accurate assessment.

Putting this another way: if we reflected more on our work, maybe we would be
able to better handle its latent side effects, its risks. But the modern world
with its focus on efficiency actively discourages us from doing that. And this
is how slow technology suggests a new way forward: arguing that we need to
design technology with the very explicit goal of inhibiting System 1 from its
instinctive reactions, and encouraging us to reflect more; in effect, designing
systems to reduce the latent side effects of our worst, reactive instincts. 

So a slow technology future is both modernist and anti-modernist at the same
time. It is modern in that it is consciously about improvement and progress, and
yet it is anti-modern in that it is a deliberate attempt to protect from the
harms of modern life. This creates some challenges for slow technology. As Beck
and Giddens argue, reflexive modernity is not always a bad thing, so pushing
back against modernization for the sake of it — which much slow technology tends
to do — can tend to throw out the baby with the bathwater. 

A second issue is that casting fast versus slow as a binary framing can be
actively regressive (Vostal, 2019). A ’slow’ tradition against ‘fast’ modernity
can be used to argue for reversing some of the positive achievements of
modernization — and there are many, ranging from improved healthcare, through
equal rights, to improved employment conditions, to name but three. Although, it
has to be said, this is perhaps more a criticism of the ‘slow’ framing than the
focus on experience that it was created to represent. 

In artificial intelligence, there is not much evidence of a slow movement.
Mirielle Hildebrandt has called for it, and some others. 

https://twitter.com/mireillemoret/status/1385935062709985284

The argument here is that slow technology is an appropriate reaction to the
Silicon Valley (and especially Facebook) mantra of “move fast and break things”.
In fact, moving fast and breaking things — while it is a very nice way of making
risk acceptable — can go too far. 

Having said that, more recent deployments of artificial intelligence are very
much in the “move fast and break things” model. ChatGPT, for example, by the
very nature of its interface, appears to trigger reflex responses — and perhaps
discourage deeper reflection. In Kahneman’s term, issues don’t get referred to
System 2. In this sense, all slow technology is really about is: invoking System
2 more than might happen otherwise. In its absence, System 1 is a less-reliable
mix of heuristics and biases that just happens to work well, and fast, most of
the time. But it’s also our human vulnerability: political manipulation (as in
the Cambridge Analytica case, for example) invariably targets System 1. If
ChatGPT behaves similarly — if it short-circuits System 2 — it introduces a
significant risk that we are less able to respond to its errors. The same is
true for image-based generative technologies: somehow we need to take explicit
action to bring reflection into our reactions, and it does not happen by
default.

## Fast technology

Others, however, reject the anti-modernism of the slow moment, and advocate
accelerationism. Instead of slowing thoughtful modernization, they advocate not
just moving fast and breaking things, but, instead, going as fast as you can to
intentionally break everything, in the hope that what emerges from the chaos is
an improvement. This is an ideology that is adopted, in different forms, by
people ranging from Peter Thiel to Karl Marx. It is particularly common in
Silicon Valley entrepreneurs and investors, especially those who are keen to
push for social change in one form or another. 

It is hard to argue against this approach, although it is ironic that many of
those who espouse some version of the ‘existential risk’ rhetoric also stand for
accelerationism, which intentionally increases those risks.

## The surveillance society

Another possible future is a new, truly modern, Panopticon. Bentham’s original
Panopticon was a physical place where transparency provided a shield against
corruption. It was Bentham’s answer to *quis custodiet ipsos custodes*: “who
guards the guards themselves?”

But our new Panopticon is a modern one, using the technology of reflexive
modernity. Where Bentham’s Panopticon  was a physical building, our technology
has flipped it -- our modern Panopticon is installed on our computers and
cellular phones, and into the fabric of our cities, stores, and financial
institutions. In this modern Panopticon we voluntarily trade our privacy for
money and other benefits. We live in what Shoshana Zuboff eloquently calls
“surveillance capitalism”.

And this is where we have, once again, lost the profound reality of Bentham’s
point. In Bentham’s Panopticon, not only could the guards see us, we could see
them. That was how Bentham solved the problem of corruption. In today’s
Panopticon — the one that lives in our cellphones and stores -- we can no longer
see the guards. Do the guards even exist? We may never truly know whether anyone
is watching us -- and this is the point, a clear thread in surveillance
societies everywhere today -- it is the *anticipation* of being watched that
modifies our behaviour, more than actually being watched. Or, more accurately,
they manufacture a new risk -- the risk of “being caught” not conforming -- and
it is the anticipation of that risk shapes our behaviour. These services, in the
limit, are literally watching us inside our own homes. They are the descendants
of Orwell’s telescreens in *1984*: they manufacture the risk of non-compliance
with authority, and induce in us an anticipation of that risk -- that us what
modifies our behaviour. 

Technologies like Palantir and Clearview AI (both have Peter Thiel investing)
exemplify another side of this modern surveillance concept. They harvest digital
data from any of our digital footprints and traces that they can legitimately
source, and use it to provide information about us — both factual and inferred —
which they sell to authorities such as governments and police forces. But, *quis
custodiet ipsos custodes?* Who guards society against Palantir and Clearview AI?
The answer today is, it seems, no-one. What passes for regulation of their
services is a patchwork of privacy laws, oversight systems, and social pressures
that prevent them -- to a degree -- from doing whatever they want. Bentham’s
answer -- transparency, the symmetric risk to those in power that they can also
be seen -- no longer holds. They exemplify a modern, privateer-led, Panopticon:
one without walls. The new Panopticon is a simulacrum of the old. 

Palantir and Clearview AI are far from alone. “Bossware” is a whole market
segment these days — especially in a world still recovering from the COVID-19
pandemic. Many employees still work from home, and these technologies, which
range from time-tracking and key-logging to camera-based monitoring at home, aim
to provide corporate managers with a detailed insight into employee behaviour.
And, similarly in education, “proctoring” services that visually monitor
students working on an online examination, constantly checking for evidence of
‘cheating’. 

Companies don’t need to go as far as using a camera to create these risks. Uber
and similar ride-sharing companies, and related delivery companies, use
location-tracking to achieve a very similar effect. If a driver waits too long
in the wrong place — or where the technologies location tracking GPS assesses as
the wrong place — they can be suspended, often automatically. 

Given that modernization has created a web of technologies that allow almost our
every single act to be tracked, stored, and used to make inferences about us,
this new surveillance society is a real and present aspect of modern life.
What’s perhaps most surprising is that we tolerate it so well. There is some
resistance, to be sure — especially in Europe where regulations like GDPR are
actively enforced against some of these behaviours. 

## Modernization as ecological succession

It is easy to frame modernization as a linear process of gradual improvement,
and looked at from a distance, it does seem to resemble that. Transporting goods
started with people, then horses, then carts, then canals, railways, and planes.
Each represented an incremental improvement that made the previous approaches
less attractive, and therefore superseded them.

Another way to think about this is like evolution. People are constantly
experimenting with different approaches, and if they work, they’re more likely
to be repeated than if they aren’t. People selected better approaches to
transporting goods. And if they worked well in one context, people might try
them in different ones. Canals were, if you think about it, a way to apply
successful methods for transporting goods on the water, but adapted to land. 

But ecological succession is not linear progression -- there is no ‘forwards’,
it is entirely common for different areas to form wholly different environments
and, therefore, shape life in a different way. 

Or think about human communications. We’ve evolved from face-to-face
communications, through postal systems, the telegraph and the telephone, to
today’s email, internet messaging, Zoom calls and the metaverse. And yet,
through COVID times and lockdowns, we transited through work from home and back
into office meetings — to some extent, anyway. In cities and large
organizations, the selection pressures are very different to those in remote
areas and small companies, so the two have begun to diverge. The metaverse
briefly seemed popular, and recurs from time to time, but despite these spurts
of intense and widespread public interest, it remains niche. There can be
environments where it succeeds, but maybe we just haven’t seen enough of them
yet.

This isn’t miles away from evolution. 

There are even ‘fossils’ in human communications — email shows a clear
structural ancestry from the office memorandum, even though today few might
recognize one. There is no deeply fundamental reason why emails need to have
“To:”, “CC:”, and “Subject:” lines, but they’re good enough, they work, so these
characteristics are retained. In fact, human communication is littered with the
fossils of old media: calling cards, ticker tape, postcards, pamphlets,
pneumatic tubes, Morse code, fax machines, and newspapers. These are relics for
most people today, and yet, we’d recognize them. 

It shouldn’t be a surprise that there is an ecological aspect to the way
technology changes over time — it’s a simple consequence of two factors:
selection and environments. First, selection: technologies are chosen and used
by people, and the more successful they are, the more likely they are to be used
again in the future. Second, environments: what works or fails in one
environment does not necessarily work or fail in another environment. For
example, fax machines are still common in healthcare because they are hard to
intercept, and a very fast way to transmit all kinds of documents. That makes
them almost completely useless for publishing information to a large group.
However, in the legal field in the UK, a private mail courier system — DX
(standing for “document exchange”) — emerged for similar document transmission,
but by and large it was a purely physical network. Different environments ended
up selecting different technologies, even though their needs were in many ways
parallel. 

In other words, modernization happens in niches, where there are opportunities.
Technologies like artificial intelligence and 3D printing can transform some
niches, while leaving others entirely unchanged. There are no guarantees as to
what will win out -- it is entirely possible for a simple and basic technology
to dominate, just as DX did. 

## Notes

* footnotes will be placed here. This line is necessary
{:footnotes}
