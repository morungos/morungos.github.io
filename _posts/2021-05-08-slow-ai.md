---
layout: post
title: 'Fed up with giant AI models? "Slow AI" is a better way'
author: Stuart
background: '/img/posts/sunset.jpg'
summary: |-
  Let's build an approach of "Slow AI", building on slow technology, as a way to
  improve people's interaction with and expriences of working with AI applications,

image: '/img/posts/sunset.jpg'
image_description: 'A calming sunset'
---

We often think of AI as a fast, disruptive technology -- one that is reshaping 
the world we live in, by changing our economic standards and values. For example, 
maybe AI can do part of a job faster and more accurately than people -- that 
opportunity for automation can transform salaries, careers, and cause the 
success or failure of entire companies.

Over the past week, I've been reconsidering that, thanks to an inspired thought
from [Mireille Hildebrandt](https://twitter.com/mireillemoret)
mentioning the idea of "Slow AI". 

<blockquote class="twitter-tweet">
<p lang="en" dir="ltr">
  And from an end-user perspective we need ‘slow AI’ as part of a human rights 
  approach, as this is very much about power asymmetries and misdirected 
  economic incentives <a href="https://t.co/zmvNxAtF42">https://t.co/zmvNxAtF42</a>
</p>&mdash; Mireille Hildebrandt (@mireillemoret) 
<a href="https://twitter.com/mireillemoret/status/1385935062709985284?ref_src=twsrc%5Etfw">April 24, 2021</a>
</blockquote>
<script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

The idea of an AI as part of the ["slow movement"](https://www.today.com/health/what-slow-movement-how-focus-little-moments-life-t163935) has an intuitive appeal. There's
even a small but distinctive field of "slow technology" -- could we establish a way
to do AI that reflects that?

[Slow technology](https://www.becktench.com/blog/2019/2/11/some-reflections-on-slow-technology) 
is the opposite of solution-oriented. It is about the experience,
not the goal. It is *envelopement*, not development -- very much the kind of
understanding and integration that 
[Madeleine Clare Elish suggests](https://mitsloan.mit.edu/ideas-made-to-matter/hidden-work-created-artificial-intelligence-programs). Slow technology is not
disruptive, instead, it's about designing to improve, and deliberate over, our
experiences.

As a psychologist, I've been thinking of slow AI through another lens, too. Kahneman, in
["Thinking Fast and Slow"](https://www.penguinrandomhouse.ca/books/89308/thinking-fast-and-slow-by-daniel-kahneman/9780385676533)
talks about people as two systems: System 1 and System 2. System 1 is the 
instinctive, reflexive side to our thinking, where System 2 is the reflective, 
deliberative side. 

The problem is, faced with events in the real world, System 1 is faster and often wins out.
For example, when we are trying to decide if something is true or not, System 1
often leaps to the conclusion that it is true, and only when (and if) System 2 
gets involved, may we reconsider and come to a more accurate assessment. Apply 
this to recommender systems, for example, or AI in recruitment -- how much of the 
damage of AI comes from System 1 leading on System 2.

If -- and this seems likely to me -- this is the underlying cause of the "cognitive miser" aspect of
[automation bias](https://en.wikipedia.org/wiki/Automation_bias), 
then we need to start designing AI to strengthen System 2, to encourage 
reflection. Slow AI may be the only way to overcome automation bias, as well as
the myriad other consequences of poor AI systems.

Maybe this shouldn't be a surprise to us. At Turalt, we've been intending to design
AI that encourages reflection, and maybe even slows down some tasks (like email)
to improve that deliberative, individual, aspect to social interaction. 

But bringing the idea of a slow AI gives us opportunities to make our work better. 
For example, there is a playfulness to slow technology, an artistic aspect that
encourages time to reflect. We need to start
building AI applications that focus less on the goal, and more on our experiences
living in the world we do.

And I don't think this applies solely to our users, either. All too often we 
are driven to build models too quickly. As developers, we are just as prone to 
let System 1 make our decisions for us, throwing compute at problems, rather than
thinking them through. Or using a bigger dataset in the hope that it'll somehow
create a better accuracy, when -- seen through the lens of Slow AI -- accuracy
is not the problem.

And finally, what about ethics? 
[Slow technology aligns well with ethics of care](http://epubs.surrey.ac.uk/804725/3/slow%20ethics.pdf),
although the "slow ethics" described there by Gallagher comments on the risk of 
superficiality, and defends the space for reflection in slow movement thinking.
The slow movement is grounded in experiences, in relationships, in attentiveness to the consequences
of our work. With a slow AI, ethics is not inherent or automatic, but by strengthening
our use of reflection and System 2 (which is more involved in ethics than System 1), it's at least
a step forward from current approaches to AI, which run at a pace almost designed to inhibit it.

So what do you think? Should we think more about *enveloping* AI, rather than deploying 
it, in Elish's phrasing? I'm certainly looking at ways to create that
playfulness, that craft, and that understanding of experience, further forward
in my work. 

Some articles on slow technology:

* [Beck Tench (2019). "Some Reflections on Slow Technology"](https://www.becktench.com/blog/2019/2/11/some-reflections-on-slow-technology)
* [Lars Hallnäs & Johan Redström (2001). "Slow Technology – Designing for Reflection", Personal and Ubiquitous Computing volume 5, 201–212](https://link.springer.com/article/10.1007/PL00000019)
* [William T. Odom et al. (2014). "Designing for slowness, anticipation and re-visitation: a long term field study of the photobox", CHI 2014.](https://dl.acm.org/doi/10.1145/2556288.2557178)
