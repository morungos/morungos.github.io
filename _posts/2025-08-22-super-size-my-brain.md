---
layout: post
title: "Super size my brain: self-actualization and AI"
background: '/img/posts/wile-e-coyote.jpg'
summary: >-
  This article describes how generative artificial intelligence tends to
  break apart teams, driving a wedge into the deep asymmetry between generating 
  information and using it. The result can be stressful for employees, but just
  as destructive on managers' strategy and vision.
image: '/img/posts/wile-e-coyote.jpg'
image_description: "Cascade on the River Etherow"
---

When I hear the increasing stories of the effects of large language models --
which increasingly seem to be transforming people's mental capacities in ways
that are harmful -- I wonder if there's a useful analogy with [Morgan Spurlock's
"Super Size Me"](https://www.imdb.com/title/tt0390521/)?

To put it simply, if we spend our days immersed, not in human society, but in a
soup of synthetic information, tailored to reflect whatever big technology corporations 
have stuffed into their training data, 
*what will be the impact on us?*

Is there a kind of “cognitive diet” that can be good —
or bad — for us? Can we end up consuming information in a way that slowly
transforms us, destroying our ability to function independently. In effect, are
our mental capacities malleable or not? 

And if our mental capacities are not fixed, but constantly shift according to
our context and experiences -- what if they they don't help us grow healthy,
but clog our mental arteries? Might we be significantly
harmed by being surrounded by these complex cognitive artefacts?

## Maslow's hierarchy of needs

One way to think about this is through the framework of Abraham Maslow, and
especially his "hierarchy of needs" (Maslow, 1943), today usually depicted as a pyramid
(although Maslow himself never depicted it that way). At the bottom are a number
of more basic needs (food, safety, love, self-esteem), and only once those are
being met are we able to explore higher ones. At the top was
"self-actualization"[^SA], which is most relevant here[^Maslow].

[^SA]: In fact, self-actualization wasn't Maslow's idea -- he'd learned it from
    his mentor, Kurt Goldstein, a pioneer in neuroscience, Jewish refugee from
    Nazi, and another manor influence on humanistic psychology. Maslow's version
    of self-actualization was a little different, centred more on personal
    growth.

[^Maslow]: This is the simple version of Maslow's model, which is more nuanced
    than commonly presented. Maslow himself mentioned that levels of need could
    be switched around from person to person and context to context. However, Maslow's
    point was that people who are fulfilled and happy tend to have a significant
    proportion of self-actualization in their mix.

Today, the field that uses Maslow's ideas most often is not psychology but business. His hierarchy, and the pyramid diagram that evolved within the business literature
after Maslow, are ubiquitous -- often bearing little resemblance to Maslow's original.
For example [Salesforce use the structure to describe corporate growth from
startups to unicorns and corporate
citizenship](https://www.salesforce.com/ca/blog/business-hierarchy-of-needs/).

Similarly, Keith Davis, whose *Human Relations at Work* (Davis, 1957) popularised
Maslow's ideas within business, argued (contra Maslow) that giving people too much safety -- one of Maslow's basic needs
-- was a bad thing for a business, a family, or a society:

> "The problem is faced alike by father with son, employer with personnel, and
> government with citizens. A dangerous aspect of too much security is that if it
> goes far enough to make man overly dependent on his securers, he may become
> complacently unproductive" (Davis, 1957, p26).

For Davis, all these different hierarchies have the same basic structure, and Maslow's
framework showed them how to induce motivation when and where you needed to, using the stick to remove
safety, rather than offering the carrot of self-actualization.

It's intriguing that by far and away the biggest impact of Maslow's framework
was in the business literature, where it very quickly caught on -- and the
pyramid diagrams today associated with Maslow were generated and propagated
entirely within this community. 

However when you read the articles that build on Maslow, there's an ironic
tension in the literature. It is usually aimed at managers -- and it is not a
coincidence that the pyramid resembles a corporate structure, so
self-actualization becomes the realm of managers, and basic needs that of lower
rank employees. There are many examples of this framing, like [this one in
Forbes](https://www.forbes.com/councils/forbeshumanresourcescouncil/2021/06/10/maslows-hierarchy-of-needs-in-your-organization-how-to-support-your-employees-at-every-stage/).

But I digress. Like Maslow, let us focus more on growth than on needs, and see how this framework helps 
us understand how people can become better, focusing on self-actualization.

## Self-actualization

Maslow came up with his interpretation of self-actualization by studing a fair number of people he considered self-actualizers (ranging from Einstein to
Harriet Tubman), to find out what they had in common, and what marked them apart
from the rest of us. All in all, he found about fifteen common behaviours,
summarised in left hand side of the table below. Down the right hand side, I've 
connected some of the corresponding effects of naive uptake of generative AI technology,
whether it be vibe coding or helping students write essays.

| Maslow's self-actualization               | Naive generative AI                                                   |
| ----------------------------------------- | --------------------------------------------------------------------- | 
| Efficient perception of reality           | Increasing distortion of reality, recursive environment drift[^Goudy] |
| Acceptance of self, others, and nature    | Guilt, shame, anxiety[^Chan]                                          |
| Spontaneity, simplicity, naturalness      | Greater salience of social norms                                      |
| Goal centered, long-term mission in life  | Satisficing, local optimization                                       |
| Accepting solitude without discomfort     | Constant need for social interaction                                  |
| Autonomy and independence                 | Increasing dependency on the perceptions of others                    |
| Continued freshness of appreciation       | Digital amnesia[^Deckker]                                             |
| Mystic (flow) experiences, awe            | Diminishing cognitive control and attention regulation[^Deckker]      |
| Gemeinschaftsgegühl (community feeling)   | Loss of community boundaries and identities, individualization        |
| Profound interpersonal relations          | Larger but shallower social networks                                  |
| Democratic character; humility, justice   | Halo effects, judgements are shaped by overall perceptions            |
| Discrimination between good and evil      | Moral relativism, [utilitarian ethics](https://www.sciencedirect.com/science/article/abs/pii/S0022103122000464)             |
| Philosophical, un-hostile sense of humour | Satire, ["punch down" comedy, stereotyping](https://www.bbc.com/future/article/20240724-can-artificial-intelligence-be-genuinely-funny)                                           |
| Child-like creativity                     | [Creativity shaped by other people's perceptions and expectations](https://link.springer.com/article/10.1007/s00146-025-02341-7)      |
| Resistance to enculturation               | "Fast culture", rapidly-evolving, viral, imitative                    |
{:.table.table-striped}

[^Goudy]: See Goudy (2025).

[^Chan]: See Chan (2025).

[^Deckker]: See Deckker & Sumanasekara (2025)

Now, as a mostly careful cognitive scientist, I will be careful not pin my
argument only on Maslow's work, which was distinctly sloppy by today's
standards[^method]. Fortunately, I don't need to. One more respectable related concept is
*need for cognition*, a personality-linked behavioural disposition (Cacioppo &
Petty, 1982), which uses psychometrics to measure individual differences. 

[^method]: Criticisms of Maslow's work have tended to focus more on his basic needs (physiological,
    safety, love and belonging, and self-esteem), and particularly their ordering, and less on
    sef-actualization, which actually held up surprisingly well in other studies.

Also, Maslow's original (1943) article is a little more nuanced than most might
think. It is not the same as it is usually presented -- especially in the business
literature. Read it as a reaction against Freudian determinism, and an early
step towards what would eventually become humanistic psychology, not as a naive
model of motivation. In that light, it holds up fairly well.

To Maslow, remember, the exploration of self-actualization was all
about personal growth: "the desire to become more and more what one is, to
become everything that one is capable of becoming" (Maslow, 1943). Instead, when
you look at the business literature, much of it reads like senior managers
yelling at their more junior employees: *"grow, damn you!"* The "self" part of
"self-actualization" is completely missing. Staff cannot be counted on to 
self-actualize, it is the manager's job to guide them into it.[^fairer]

[^fairer]: A fairer interpretation of Maslow for business would be 
    that employers shoild focus on minimizing basic needs: keeping their employees well-fed,
    safe and healthy, comfortable with their work-life balance, and respected by their
    colleagues. That done, they'd get on and self-actualize themselves
    -- unless the company actively prevented it. Maslow would have approved of
    good health insurance, remote working, and the like. He'd frown on
    modern versions of scientific management, using metrics for performance 
    assessment and the like, as they wield basic needs instead, and therefore
    limit rather than enable growth. His view was, essentially, that a happy
    person will motivate themselves, they won't need to be motivated by others.

## Self-actualization versus self-image actualization

In a rare moment of insight, Fritz Perls (Perls was a bit of an asshole, to be
honest) highlighted an issue with this framework that
Maslow had missed. Perls said: "Many people dedicate their lives to
actualize a concept of what they *should* be like, rather than to actualize
*themselves*" (Perls, 1969, p20, original emphasis).

This is a crucial distinction: Maslow's focus was on people growing, becoming all that they
could be. But, all to easily, a need for esteem, or other needs, provides a
kind of saccharine substitute, a simulation of growth. 

To illustrate, check out [this
article: "Does being 'manly' make you healthier and happier?"](https://www.cbc.ca/documentaries/the-passionate-eye/does-being-manly-make-you-healthier-and-happier-the-top-5-reasons-the-answer-is-nope-1.7358633)
and the accompanying CBC documentary [*Harder Better Faster
Stronger*](https://gem.cbc.ca/harder-better-faster-stronger). Remember: Maslow's
point is essentially that happiness and fulfillment come with
self-actualization. This documentary shows the harms of actualizing self-image
(in this case, an image as a "real man"). There are some intriguing parallels
with generative AI in the table above, all too easily guilt, shame, and anxiety
replace the acceptance of self and others in true self-actualization.

There is a pattern: when self-image actualization replaces
self-actualization, it results in psychological harm -- in ways that are remarkably
similar to the unbridled consequences of AI usage. In all cases, not only do we stop
growing, we regress.[^positive]

[^positive]: There is an argument that this is a significant risk within all
    approaches broadly within the field of modern positive psychology, which is
    a little ironic given that Maslow called his approach "positive psychology",
    predating and significantly inspiring Seligman's and others work.

## Artificial intelligence as a dark mirror

One of the few true experts in both artificial intelligence and in
psychoanalytic thinking is Sherry Turkle. Her classic, *The Second Self*
(Turkle, 1984), explores the mirror as a metaphor for intelligent machines (for
want of a better term), in some depth. One passage is worth quoting in full:

> Ours has been called a culture of narcissism. The label is apt but can be
> misleading. It reads colloquially as selfishness and self-absorption. But
> these images do not capture the anxiety behind our search for mirrors. We are
> insecure in our understanding of ourselves, and this insecurity breeds a new
> preoccupation with the question of who we are. We search for ways to see
> ourselves. The computer is a new mirror, the first psychological machine.
> Beyond its nature as an analytical engine lies its second nature as an
> evocative object. (Turkle, 1984, p279).

Turkle's insight completes the puzzle. As we see ourselves in the mirror of new
artificial intelligence, we all too easily see a false self-image -- ourselves
as we wish to be seen: creative, autonomous, rich, powerful, beautiful, loved.
We *want* our super-sized brain -- and, more importantly, we *need* everyone else
to see it.

<figure>
</figure>

And all too easily, ChatGPT and its fellows, the evocative mirrors of our new
technological world, persuade us that this self-image is real. That we are
right, smart, creative, and talented. Whether or not this is by design, it
happens: [psychological mirroring and boundary dissolution were two of the most
significant patterns identified by Goudy](https://zenodo.org/records/16879563)
in her analysis of maladaptive human-AI relationships (Goudy, 2025).

And when it does happen, when we start to lose the more genuine self-awareness
we learn through interaction with family and friends, colleagues and
strangers, then generative AI -- in its current dominant form as a tool -- is
directly harmful to self-actualization.

## One alternative: slow AI

Of course, there are other ways the technology can work -- Slow AI is one example, 

> [There] is a playfulness to slow technology, an artistic aspect that encourages 
> time to reflect. We need to start building AI applications that focus less on 
> the goal, and more on our experiences living in the world we do.

The problem is, slow AI is harder to design and to build. it requires thought
and skill to help build an experience that goes beyond this illusion -- and,
oddly, the kinds of skills that great artists and great game designers often
excel at. I want the technology experiences of a Banksy, not a Musk.

## Another alternative: stories

Another approach is to step away from reflecting one person, and instead show
reflections of other -- real -- people. That was the basis of Roger Schank's 
"ASK Systems". One great example was the "Sounding Board" -- it had a chat-like
interface but instead of using generative text to tell you what to do, it 
presented short video clips of what other real people had done, maybe even in
slightly different situations. 

This changes the way we perceive the information: we have to work, cognitively,
to apply the information. And we get to see the value in other people's
expertise, so we can see the value our own more accurately. 

## Generative artificial intelligence:

So, will generative artificial intelligence make us into cognitive couch
potatoes? Will it exert a pressure that makes self-actualization harder? 
There hasn't been a whole lot of research on this, but I think there is a 
case to answer.

In a way, it all comes down to the basic question we asked at the start: are our
mental capacities malleable or not? 

If they aren't, then generative artificial intelligence is 

But if they are, if we need to rise towards self-actualization to be happy 


## References

[Cacioppo, J. T., & Petty, R. E. (1982). The need for cognition. *Journal of
Personality and Social Psychology*, **42**(1), 116–131.](https://doi.org/10.1037/0022-3514.42.1.116)

[Chan, C. K. Y. (2025). Understanding AI guilt: The development, pilot-testing,
and validation of an instrument for students. *Education and Information
Technologies*.](https://doi.org/10.1007/s10639-025-13629-y) 

Davis, K. (1957). *Human relations at work* (2nd ed., 1962). McGraw Hill.

[Deckker, D., & Sumanasekara, S. (2025). A Systematic Review of the Impact of
Artificial Intelligence, Digital Technology, and Social Media on Cognitive
Functions. International Journal of Research and Innovation in Social Science,
IX(III), 134–154.](https://doi.org/10.47772/IJRISS.2025.90300011)

[Goudy, A. (2025). the entanglement spiral: an exploratory framework for
recursive entanglement drift in Human-AI relationships.](https://zenodo.org/records/16879563)

[Maslow, A. H. (1943). A theory of human motivation. *Psychological Review*,
**50**(4), 370–396.](https://doi.org/10.1037/h0054346)

Schank, R. C. (1999). *Dynamic memory revisited*. Cambridge University Press.

Turkle, S. (1984). *The second self: computers and the human spirit*. Simon & Schuster.

## Notes


Recursive environment drift
* https://www.psychologytoday.com/us/blog/the-algorithmic-mind/202508/how-ai-chatbots-may-blur-reality
