---
layout: post
title: "LLMs: intelligence amplification doesn't work on stupid people"
author: Stuart
background: '/img/posts/kearney-lake.jpg'
summary: >-
  Giving a fool a large language model simply makes them faster
  at their foolishness, it does not make them equal to 
  a skilled or experienced person.
image: '/img/posts/kearney-lake.jpg'
image_description: |-
  A small forested lake, surrounded by scrubland.
---

Sometimes, people who had shown a glimmering of understanding, manage to
demonstrate that they too do not get it. Today's example is from Sharknado III's
President himself, Mark Cuban.

<blockquote class="bluesky-embed" data-bluesky-uri="at://did:plc:y5xyloyy7s4a2bwfeimj7r3b/app.bsky.feed.post/3lifd5rbnkk2k" data-bluesky-cid="bafyreifrhil5cljvrehmuemxujophwn55pl5ou5qa5wgwjkghis33h7gca"><p lang="en">If you have zero education, but learn how to ask AI models the right questions , in many jobs you will be able to outperform someone with an advanced degree, but who is unwilling to use Large Language Models. 

Just takes a smartphone, curiosity to experiment and a mindset to learn.</p>&mdash; Mark Cuban (<a href="https://bsky.app/profile/did:plc:y5xyloyy7s4a2bwfeimj7r3b?ref_src=embed">@mcuban.bsky.social</a>) <a href="https://bsky.app/profile/did:plc:y5xyloyy7s4a2bwfeimj7r3b/post/3lifd5rbnkk2k?ref_src=embed">February 17, 2025 at 1:59 PM</a></blockquote><script async src="https://embed.bsky.app/static/embed.js" charset="utf-8"></script>

[Frederick Brooks, in 1996, wrote a wonderful and thoughtful
article](https://www.cs.unc.edu/~brooks/Toolsmith-CACM.pdf)[^Brooks] on the framing of
computing, focusing especially on "computer science" and "artificial
intelligence". And by 'framing' here, I mean, what conceptual framework helps us
to understand how these fields work. 

[^Brooks]: Brooks, F. (1996). The Computer Scientist as Toolsmith II.
    *Communications of the ACM*, March 1996, **39**(*3*), pp. 61-68. This
    article was based on his acceptance lecture for the first ACM Allen Newell
    Award, which was given in 1994.

Many of the challenges in the broad field of computing are analyzed in this
article: is it science or engineering? Is it about discovery or creation?
Brooks's answer is, computing is broadly about *making tools*, we are toolmakers,
nothing more, and nothing else. "An honourable calling", in his terms.

But there's also some Tolkein in there. Brooks builds on Tolkein, following
Dorothy Sayers, and describes what computing folks do as "*subcreation*" -- we
create, but only within the framework within which we ourselves are made.

A large language model is one of these tools, and as such, it exists entirely
within the framework of human language and human culture. It is a subcreation,
not a source of universal wisdom. The question is, what does this kind of tool
help us to do?

Brooks moves on to discuss how contemporary AI (this was 1996, remember) had
evolved in a positive way, from the original and sketchy goal of building giant
brains into something else: building tools that were *assistants and advisors*.
Instead of attempting to 'be' an entire entity (as the 'general AI' folks would
have it), they help people. 

Brooks's thesis was:

> IA (intelligence amplifying) > AI (artificial intelligence)

If what we are building, and what LLMs are, is about *amplifying* human
intelligence, how does this reflect on Cuban's point?

Well, let's re-frame large language models as an *amplifier*[^Counterpoint]. 

[^Counterpoint]: Now you might disagree. Consider: image generation. Is it not
    actually creating images? Well, no. it's using a *prompt*, and those who
    embrace generative AI embrace these prompts as being critical to a good
    outcome. An image generation AI is *amplifying* a prompt into an image.

A person with zero knowledge of a task, with an LLM, might perform marginally
better than a person of zero knowledge without an LLM. But they will always be
substantially off worse than a person with good knowledge of the task, 
probably with or without an LLM. 

A good coder will *always* produce better code than a non-coder with an LLM. 

What an LLM might bring is *productivity*. A non-coder with an LLM might well be
more productive than a non-coder without, but they are inevitably going to be
inferior to a good coder with an LLM. The same goes for a writer, an accountant,
a graphic designer, and even, possibly, a CEO. 

In other words, a large language model will not help a stupid person to become
smart. It's a tool, an amplifier -- it simply enables them to use what
capacities they do have, more productively. It simply makes that stupid person
capable of generating exponentially more stupidity. 

That's the point about an amplifier: if there isn't any signal, all you get is noise 
-- white noise[^WhiteNoise][^GIGO]. 

[^WhiteNoise]: [A lovely and ironic pun, entirely due to Andrew
    Robinson](https://bsky.app/profile/andrewr.bsky.social/post/3lifqufbkv226).
    And meaningful, too: as [language model outputs tend to be biased to the
    stereotypical](https://morungos.com/2022/01/09/language-model-bias/).  

[^GIGO]: An even older computing person might simply describe this as: garbage
    in, garbage out. And they'd not be wrong.

Seeing ourselves as toolsmiths, Brooks argues, also changes the way we need to
work. 

> "If the computer scientist is a toolsmith, and if our delight is to fashion
> power tools and amplifiers for minds, we must partner with those who will use
> our tools, those whose intelligences we hope to amplify" (Brooks, 1996, p64).

Brooks's argument also matches my personal experience: we should never foist our
tools onto users, we should collaborate with them. In fact, that is how we learn
to be better. Working with other people, on their problems, helps us: we get to
work on challenging, interesting, and real problems, not toy ones; it keeps us
honest and away from hype; and, not least -- it is more fun. 

## Notes

