---
layout: page
title: AI and technology for assessment in higher education
background: '/img/bg-about.jpg'
---

When I worked in academia, one of the areas I was early involved with
was using artificial intelligence in assessment. 

My first encounter with this was thanks to one of our senior managers, 
who asked me to look into the automated essay grading technology of the 
time, and particularly ETS's work on e-rater, and its basis in latent semantic 
analysis.

In parallel with this, I'd had a very long involvement with automated surveying.
I started using web based surveying almost as soon as forms were included in 
HTML, back in the early days of HTML, while Netscape Navigator was the new kid
on the block. The only way to capture data then was through CGI scripts, but
all they really needed to do was append data to a big text file, and that was
enough to make data recording from surveys work well. The web was still very
new then, so this was novel enough that response rates were very high.

We used this for a few different projects, but the real advantage was in 
evaluation, especially where I worked at the UK's Open University. Course evaluation
there was a big deal -- there was a decent-sized team of full-time staff, and a
full evaluation cycle took a matter of months to collect the data, analyze it,
generate the reports, and send them for review.

## Annotated bibliography

An analysis of the language of feedback, using an interaction model. The short
version is: we found very strong systematic associations between certain
*categories* of comment (e.g., asking questions) and the derived grade. This
allowed us to develop later tools that help support and train tutors, for
example: if a tutor asked too many questions it might not give the same
impression as the grade awarded. This kind of AI -- designed only for helping
tutors learn to give quality feedback -- remains under-explored within
educational technology.

> Whitelock, D. M., Watt, S. N., Raw, Y., & Moreale, E. (2003). Analysing tutor
> feedback to students: First steps towards constructing an electronic
> monitoring system. *ALT-J*, *11*(3), 31–42.
> [https://doi.org/10.1080/0968776030110304](https://doi.org/10.1080/0968776030110304)

This article describes a method for surveying students and automating the
generation of high-quality course evaluation reports. Its most significant
technical innovation was a reporting system that integrated narrative text,
analysis, and charting, so that high-quality reports could be produced in
seconds. We assessed this approach comparing its advantages and disadvantages to
traditional surveying.

> Watt, S. N., Simpson, C., McKillop, C., & Nunn, V. (2002). Electronic course
> surveys: Does automating feedback and reporting give better results?
> *Assessment & Evaluation in Higher Education*, *27*(4), 325–337.
> [https://doi.org/10.1080/0260293022000001346](https://doi.org/10.1080/0260293022000001346)
> ([*Preprint available here*](/assets/ECS.d3.pdf))
