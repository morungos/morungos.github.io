---
layout: post
title: A proxy server for developing and debugging PHP apps under FPM
author: Stuart
background: '/img/posts/angry-cat-laptop.jpg'
summary: |-
  
image: '/img/posts/angry-cat-laptop.jpg'
image_description: "An angry cat sitting on a laptop"
---

I've been working away curating data for a small project, and it's been intriguing.

The project -- while I will be talking about more soon -- involves a relatively 
small character-based language model, developed using a fairly standard recurrent
neural network. And in fact, the actual techology used is not especially relevant.
So what gives?

1. Temperature drives bias

Many generative models include a parameter to guide how "risky" predictions can
be. In a character-ased RNN this is usually a temperature value, and the lower 
the temperature, the safer (in theory) the predictions.

What this actually does is dial down the probability distribution, so that 
higher probabilities in the model (and, therefore, in the data, are preferred).
One part of our model generates people's names, and see the effect on the 
number of names generated called "John". 

[Graph]

This will also affect 

2. It's not a case of if you get offensive content, it's when you

Being a probabilistic model, at the character level, it creates novel words
even when they aren't in the training data. So it will create offensive words,
eventually, it's just depends on the probabilities. 

So, allow me to introduce you to one of the people we generated: "Albert Fucking Green".
The "F" word is not in the training data, I promise. However, "Fu" is a relatively common
name start, "uc" is common, so is "ck", and so on. Even a naive Markov chain model would
create Mr Green's name easily.

It gets worse. Obviously racist epithets are just as likely.

So, how do you handle offensive content that is not in the training data? 

In effect, you need some system for detecting a removing it. Ideally a human, but a decent
set of filters might also be enough. For a small-scale project, like this one, a set of
filters will suffice. I curated a set from the Ofcom code of bad words (those classified above "mild")
with a metaphone layer to handle soundalikes and stemming to increase detection.

Note that being generative, we are free to bias to sensitivity over specificity, because
we can freely generate many more candidates. 

Note that this contains *extremely bad language* and will probably ring alarm bells in
everyone's office if you open it at work. 

https://www.ofcom.org.uk/__data/assets/pdf_file/0020/225335/offensive-language-quick-reference-guide.pdf

Some of these, obviously, are contextual, but since we are generating names, 
we can opt to be better safe than sorry, and exclude all. 

Also, some of these are actual names, so poor Richard and Frances are no longer
going to be allowed to use some of their abbrevations -- even though Dick Cheney 
Fanny Burney are both in the training data.

